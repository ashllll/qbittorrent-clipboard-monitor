"""
åŸºäº crawl4ai çš„ç½‘é¡µçˆ¬è™«æ¨¡å—

ä½¿ç”¨ä¸“ä¸šçš„ crawl4ai åº“è¿›è¡Œç½‘ç«™æŠ“å–å’Œå†…å®¹æå–
"""

import asyncio
import logging
import random
import re
import urllib.parse
import time
import hashlib
from collections import defaultdict, deque
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional, Set, Any, Tuple
from dataclasses import dataclass, field
from pathlib import Path

from crawl4ai import AsyncWebCrawler, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy, CosineStrategy

from .config import AppConfig
from .qbittorrent_client import QBittorrentClient
from .ai_classifier import AIClassifier
from .notifications import NotificationManager
from .utils import parse_magnet, validate_magnet_link
from .exceptions import CrawlerError
from .resilience import RateLimiter, CircuitBreaker, LRUCache, MetricsTracker


@dataclass
class TorrentInfo:
    """ç§å­ä¿¡æ¯æ•°æ®ç±»"""
    title: str
    detail_url: str
    magnet_link: str = ""
    size: str = ""
    seeders: int = 0
    leechers: int = 0
    category: str = ""
    status: str = "pending"  # pending, extracted, added, failed, duplicate
    
@dataclass
class CrawlerStats:
    """çˆ¬è™«æ€§èƒ½ç»Ÿè®¡"""
    requests_made: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    errors: int = 0
    response_times: deque = field(default_factory=lambda: deque(maxlen=100))
    circuit_breaker_trips: int = 0
    rate_limit_hits: int = 0
    
    def get_avg_response_time(self) -> float:
        """è·å–å¹³å‡å“åº”æ—¶é—´"""
        return sum(self.response_times) / len(self.response_times) if self.response_times else 0.0
    
    def get_cache_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0


class CrawlerResourcePool:
    def __init__(self, max_size: int, config: AppConfig, logger: logging.Logger):
        self.max_size = max_size
        self.config = config
        self.logger = logger
        self._pool: List[AsyncWebCrawler] = []
        self._semaphore = asyncio.Semaphore(max_size)

    async def acquire(self) -> AsyncWebCrawler:
        async with self._semaphore:
            if self._pool:
                return self._pool.pop()
            crawler = AsyncWebCrawler(
                headless=True,
                browser_type="chromium",
                verbose=False,
                delay_before_return_html=2.0,
                js_code=[
                    "window.scrollTo(0, document.body.scrollHeight);",
                    "await new Promise(resolve => setTimeout(resolve, 1000));"
                ]
            )
            await crawler.start()
            return crawler

    async def release(self, crawler: AsyncWebCrawler):
        if len(self._pool) < self.max_size:
            self._pool.append(crawler)
        else:
            await crawler.close()

    async def close_all(self):
        while self._pool:
            crawler = self._pool.pop()
            try:
                await crawler.close()
            except Exception as exc:
                self.logger.warning(f"å…³é—­çˆ¬è™«å®ä¾‹æ—¶å‡ºé”™: {exc}")


class WebCrawler:
    """åŸºäº crawl4ai çš„ç½‘é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, config: AppConfig, qbt_client: QBittorrentClient):
        self.config = config
        self.qbt_client = qbt_client
        self.logger = logging.getLogger('WebCrawler')
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.ai_classifier = AIClassifier(config.deepseek)
        self.notification_manager = NotificationManager(config.notifications.model_dump())
        
        # è¿æ¥æ± é…ç½®
        self._connection_pool_size = getattr(config.web_crawler, 'connection_pool_size', 5)
        self._resource_pool = CrawlerResourcePool(self._connection_pool_size, config, self.logger)
        
        # ç¼“å­˜ç³»ç»Ÿ
        cache_size = getattr(config.web_crawler, 'cache_max_size', 1000)
        cache_ttl = getattr(config.web_crawler, 'cache_ttl_seconds', 3600)
        self._cache = LRUCache(max_size=cache_size, ttl_seconds=cache_ttl)
        
        # æ€§èƒ½ç›‘æ§
        self._performance_stats = CrawlerStats()
        self._metrics = MetricsTracker()
        
        # æ–­è·¯å™¨é…ç½®
        self._circuit_breaker = CircuitBreaker(
            failure_threshold=getattr(config.web_crawler, 'circuit_breaker_threshold', 5),
            recovery_timeout=getattr(config.web_crawler, 'circuit_breaker_recovery_seconds', 300),
            on_state_change=self._on_circuit_state_change,
        )
        
        # é€Ÿç‡é™åˆ¶
        self._rate_limit_requests = getattr(config.web_crawler, 'max_requests_per_minute', 60)
        self._rate_limiter = RateLimiter(self._rate_limit_requests)
        
        # çº¿ç¨‹æ± æ‰§è¡Œå™¨
        self._thread_pool = ThreadPoolExecutor(max_workers=4)
        
        # æ¸…ç†çŠ¶æ€æ ‡å¿—
        self._is_cleaned_up = False
        self._cleanup_lock = asyncio.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        self.stats = {
            'pages_crawled': 0,
            'torrents_found': 0,
            'magnets_extracted': 0,
            'torrents_added': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }
        
        self.processed_hashes: Set[str] = set()
    
    async def _get_crawler_from_pool(self) -> AsyncWebCrawler:
        return await self._resource_pool.acquire()
    
    async def _return_crawler_to_pool(self, crawler: AsyncWebCrawler):
        await self._resource_pool.release(crawler)
    
    def _get_cache_key(self, url: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{url}:{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        cached = self._cache.get(cache_key)
        if cached is not None:
            self._performance_stats.cache_hits += 1
            return cached
        self._performance_stats.cache_misses += 1
        return None
    
    def _put_to_cache(self, cache_key: str, data: Any):
        self._cache.set(cache_key, data)
    
    def _check_rate_limit(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶"""
        allowed = self._rate_limiter.allow()
        if not allowed:
            self._performance_stats.rate_limit_hits += 1
        return allowed
    
    def _on_circuit_state_change(self, state: str):
        if state == 'open':
            self._performance_stats.circuit_breaker_trips += 1
            self.logger.warning("Crawler circuit breaker opened")
        elif state == 'half_open':
            self.logger.info("Crawler circuit breaker half-open")
        elif state == 'closed':
            self.logger.info("Crawler circuit breaker closed")
    
    def _check_circuit_breaker(self) -> bool:
        """æ£€æŸ¥æ–­è·¯å™¨çŠ¶æ€"""
        return self._circuit_breaker.allow()
    
    def _record_success(self):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        self._circuit_breaker.record_success()
    
    def _record_failure(self):
        """è®°å½•å¤±è´¥è¯·æ±‚"""
        self._circuit_breaker.record_failure()
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'requests_made': self._performance_stats.requests_made,
            'cache_hits': self._performance_stats.cache_hits,
            'cache_misses': self._performance_stats.cache_misses,
            'cache_hit_rate': self._performance_stats.get_cache_hit_rate(),
            'avg_response_time': self._performance_stats.get_avg_response_time(),
            'errors': self._performance_stats.errors,
            'circuit_breaker_trips': self._performance_stats.circuit_breaker_trips,
            'circuit_breaker_state': self._circuit_breaker.state,
            'rate_limit_hits': self._performance_stats.rate_limit_hits,
            'cache_size': len(self._cache),
            'pool_size': self._connection_pool_size
        }
    
    async def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        async with self._cleanup_lock:
            if self._is_cleaned_up:
                return
            
            self.logger.info("å¼€å§‹æ¸…ç†WebCrawlerèµ„æº...")
            
            try:
                # å…³é—­è¿æ¥æ± ä¸­çš„æ‰€æœ‰çˆ¬è™«
                await self._resource_pool.close_all()
                self.logger.debug("çˆ¬è™«è¿æ¥æ± å·²æ¸…ç†")
                
                # æ¸…ç†ç¼“å­˜
                if hasattr(self, '_cache'):
                    self._cache.clear()
                    self.logger.debug("ç¼“å­˜å·²æ¸…ç†")
                
                # å…³é—­çº¿ç¨‹æ± æ‰§è¡Œå™¨
                if hasattr(self, '_thread_pool') and self._thread_pool:
                    self._thread_pool.shutdown(wait=True)
                    self.logger.debug("çº¿ç¨‹æ± å·²å…³é—­")
                
                # æ¸…ç†AIåˆ†ç±»å™¨
                if hasattr(self, 'ai_classifier') and hasattr(self.ai_classifier, 'cleanup'):
                    await self.ai_classifier.cleanup()
                    self.logger.debug("AIåˆ†ç±»å™¨å·²æ¸…ç†")
                
                self._is_cleaned_up = True
                self.logger.info("WebCrawlerèµ„æºæ¸…ç†å®Œæˆ")
                
            except Exception as e:
                self.logger.error(f"æ¸…ç†WebCrawlerèµ„æºæ—¶å‡ºé”™: {str(e)}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ¸…ç†"""
        if not self._is_cleaned_up:
            try:
                # åŒæ­¥æ¸…ç†å…³é”®èµ„æº
                if hasattr(self, '_cache'):
                    self._cache.clear()
                    
                if hasattr(self, '_thread_pool') and self._thread_pool:
                    self._thread_pool.shutdown(wait=False)
                    
                # èµ„æºåœ¨ cleanup ä¸­å·²å¤„ç†
                
            except Exception:
                pass  # å¿½ç•¥ææ„æ—¶çš„å¼‚å¸¸
    
    async def _make_request_with_cache(self, url: str, **kwargs) -> Optional[Any]:
        """å¸¦ç¼“å­˜çš„HTTPè¯·æ±‚æ–¹æ³•"""
        # æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if not self._check_rate_limit():
            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            if not self._check_rate_limit():
                raise CrawlerError(f"Rate limit exceeded for {url}")
        
        # æ£€æŸ¥æ–­è·¯å™¨
        if not self._check_circuit_breaker():
            raise CrawlerError(f"Circuit breaker is open for {url}")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(url, **kwargs)
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            return cached_result
        
        start_time = time.time()
        crawler = None
        
        try:
            self._metrics.inc('requests')
            self._metrics.update_last_request_time(datetime.now().isoformat())
            # ä»è¿æ¥æ± è·å–çˆ¬è™«
            crawler = await self._get_crawler_from_pool()
            
            # æ‰§è¡Œè¯·æ±‚
            result = await crawler.arun(
                url=url,
                **kwargs
            )
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            response_time = time.time() - start_time
            self._performance_stats.requests_made += 1
            self._performance_stats.response_times.append(response_time)
            self._metrics.record_response(response_time)
            
            # è®°å½•æˆåŠŸ
            self._record_success()
            
            # ç¼“å­˜ç»“æœ
            self._put_to_cache(cache_key, result)
            
            return result
            
        except Exception as e:
            # è®°å½•å¤±è´¥
            self._record_failure()
            self._performance_stats.errors += 1
            self._metrics.inc('errors')
            
            self.logger.error(f"Request failed for {url}: {str(e)}")
            raise CrawlerError(f"Failed to crawl {url}: {str(e)}") from e
            
        finally:
            # è¿”å›çˆ¬è™«åˆ°è¿æ¥æ± 
            if crawler:
                await self._return_crawler_to_pool(crawler)
     
    async def crawl_xxxclub_search(self, search_url: str, max_pages: int = 1) -> List[TorrentInfo]:
        """
        æŠ“å–XXXClubæœç´¢é¡µé¢
        
        Args:
            search_url: æœç´¢é¡µé¢URL
            max_pages: æœ€å¤§æŠ“å–é¡µæ•°
            
        Returns:
            ç§å­ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info(f"ğŸ•·ï¸ å¼€å§‹æŠ“å–XXXClubæœç´¢é¡µé¢: {search_url}")
        torrents = []
        
        # å¢å¼ºçš„çˆ¬è™«é…ç½®ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°
        crawler_config = {
            'verbose': False,
            'browser_type': 'chromium',
            'headless': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
                'Referer': 'https://www.google.com/',
            },
            'page_timeout': self.config.web_crawler.page_timeout,
            'wait_for': self.config.web_crawler.wait_for,
            'delay_before_return_html': self.config.web_crawler.delay_before_return,
            'proxy': self.config.web_crawler.proxy,
            'viewport': {'width': 1920, 'height': 1080},
            'timezone_id': 'Asia/Shanghai',
            'locale': 'zh-CN',
            'geolocation': {'latitude': 39.9042, 'longitude': 116.4074},  # åŒ—äº¬åæ ‡
            'permissions': ['geolocation'],
            'extra_http_headers': {
                'DNT': '1',
                'Sec-Ch-Ua': '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
            }
        }
        
        try:
            for page in range(1, max_pages + 1):
                    # æ„å»ºåˆ†é¡µURL
                    if page > 1:
                        if '?' in search_url:
                            page_url = f"{search_url}&page={page}"
                        else:
                            page_url = f"{search_url}?page={page}"
                    else:
                        page_url = search_url
                    
                    self.logger.info(f"ğŸ“„ æŠ“å–ç¬¬ {page} é¡µ: {page_url}")
                    
                    # æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼Œä½¿ç”¨é…ç½®å‚æ•°
                    max_retries = self.config.web_crawler.max_retries
                    base_delay = self.config.web_crawler.base_delay
                    success = False
                    last_error = None

                    for attempt in range(max_retries):
                        try:
                            if attempt > 0:
                                # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼Œé™åˆ¶æœ€å¤§å»¶è¿Ÿ
                                delay = min(base_delay * (2 ** (attempt - 1)) + random.uniform(0, 3),
                                          self.config.web_crawler.max_delay)
                                self.logger.info(f"ğŸ”„ ç¬¬ {page} é¡µé‡è¯• {attempt+1}/{max_retries}, ç­‰å¾… {delay:.1f} ç§’")
                                await asyncio.sleep(delay)

                            # åŠ¨æ€è°ƒæ•´çˆ¬è™«å‚æ•°
                            current_config = crawler_config.copy()
                            if attempt > 1:  # ç¬¬äºŒæ¬¡é‡è¯•åå¢åŠ ç­‰å¾…æ—¶é—´
                                current_config['wait_for'] = min(current_config['wait_for'] * 2, 15)
                                current_config['delay_before_return_html'] = min(current_config['delay_before_return_html'] * 2, 8)
                            
                            # ä½¿ç”¨ç¼“å­˜è¯·æ±‚æ–¹æ³•æŠ“å–é¡µé¢
                            result = await self._make_request_with_cache(
                                url=page_url,
                                wait_for=current_config['wait_for'],
                                js_code=None if attempt < 2 else "window.scrollTo(0, document.body.scrollHeight);",  # ç¬¬ä¸‰æ¬¡é‡è¯•åæ¨¡æ‹Ÿæ»šåŠ¨
                                css_selector="ul.tsearch li",
                                bypass_cache=True,
                                page_timeout=current_config['page_timeout'],
                                delay_before_return_html=current_config['delay_before_return_html'],
                                extra_http_headers={
                                    'X-Requested-With': 'XMLHttpRequest' if attempt > 1 else None
                                }
                            )
                            
                            if result.success:
                                success = True
                                break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                            else:
                                error_msg = str(result.error_message) if result.error_message else "Unknown error"
                                if "ERR_CONNECTION_RESET" in error_msg or "Connection reset" in error_msg:
                                    self.logger.warning(f"âš ï¸ ç¬¬ {page} é¡µè¿æ¥è¢«é‡ç½®ï¼Œå°è¯• {attempt+1}/{max_retries}")
                                    if attempt < max_retries - 1:
                                        continue  # ç»§ç»­é‡è¯•
                                else:
                                    self.logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæŠ“å–å¤±è´¥: {error_msg}")
                                    if attempt < max_retries - 1:
                                        continue  # ç»§ç»­é‡è¯•
                                break  # éé‡è¯•ç±»é”™è¯¯ï¼Œç›´æ¥è·³å‡º
                                
                        except Exception as e:
                            error_msg = str(e)
                            if "ERR_CONNECTION_RESET" in error_msg or "Connection reset" in error_msg:
                                self.logger.warning(f"âš ï¸ ç¬¬ {page} é¡µè¿æ¥é‡ç½®å¼‚å¸¸ï¼Œå°è¯• {attempt+1}/{max_retries}")
                                if attempt < max_retries - 1:
                                    continue  # ç»§ç»­é‡è¯•
                            else:
                                self.logger.error(f"âŒ ç¬¬ {page} é¡µæŠ“å–å¼‚å¸¸: {error_msg}")
                                if attempt < max_retries - 1:
                                    continue  # ç»§ç»­é‡è¯•
                            break  # æœ€åä¸€æ¬¡å°è¯•æˆ–éé‡è¯•ç±»é”™è¯¯
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸæŠ“å–
                    if not success:
                        self.logger.error(f"âŒ ç¬¬ {page} é¡µç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥")
                        
                        # å¦‚æœæ˜¯ç¬¬ä¸€é¡µå°±å¤±è´¥ï¼Œæä¾›ç”¨æˆ·æŒ‡å¯¼
                        if page == 1:
                            self.logger.warning("ğŸš¨ æœç´¢é¡µé¢æŠ“å–è¢«é˜»æ­¢ï¼Œå¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶")
                            self.logger.info("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                            self.logger.info("   1. æ‰‹åŠ¨è®¿é—®æœç´¢é¡µé¢ï¼ŒæŸ¥çœ‹æ˜¯å¦éœ€è¦éªŒè¯ç ")
                            self.logger.info("   2. é€ä¸ªå¤åˆ¶ç§å­è¯¦æƒ…é¡µé¢çš„ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
                            self.logger.info("   3. ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ å•ä¸ªç£åŠ›é“¾æ¥")
                            self.logger.info(f"   4. æœç´¢é¡µé¢: {page_url}")
                            
                            # æ—©æœŸé€€å‡ºï¼Œé¿å…æµªè´¹æ›´å¤šæ—¶é—´
                            break
                        continue  # ç»§ç»­å°è¯•ä¸‹ä¸€é¡µ
                    
                    # è§£æé¡µé¢å†…å®¹
                    try:
                        page_torrents = await self._parse_xxxclub_content(result, search_url)
                        torrents.extend(page_torrents)
                        
                        self.stats['pages_crawled'] += 1
                        self.logger.info(f"âœ… ç¬¬ {page} é¡µè§£æå®Œæˆï¼Œæ‰¾åˆ° {len(page_torrents)} ä¸ªç§å­")
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç§å­ï¼Œå¯èƒ½å·²ç»åˆ°æœ€åä¸€é¡µ
                        if not page_torrents:
                            self.logger.info("ğŸ“„ æ²¡æœ‰æ‰¾åˆ°æ›´å¤šç§å­ï¼Œåœæ­¢åˆ†é¡µæŠ“å–")
                            break
                    except Exception as e:
                        self.logger.error(f"âŒ ç¬¬ {page} é¡µè§£æå¤±è´¥: {str(e)}")
                        continue
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    if page < max_pages:  # ä¸æ˜¯æœ€åä¸€é¡µ
                        await asyncio.sleep(3 + (page % 3))  # 3-5ç§’éšæœºå»¶è¿Ÿ
            
            self.stats['torrents_found'] = len(torrents)
            
            if torrents:
                self.logger.info(f"ğŸ¯ æœç´¢é¡µé¢æŠ“å–å®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(torrents)} ä¸ªç§å­")
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç§å­")
                self.logger.info("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                self.logger.info("   - æœç´¢å…³é”®è¯æ— ç»“æœ")
                self.logger.info("   - ç½‘ç«™åçˆ¬è™«æœºåˆ¶é˜»æ­¢è®¿é—®")
                self.logger.info("   - ç½‘é¡µç»“æ„å¯èƒ½å‘ç”Ÿå˜åŒ–")
                self.logger.info("ğŸ’¡ å»ºè®®:")
                self.logger.info("   - æ‰‹åŠ¨è®¿é—®é¡µé¢ç¡®è®¤å†…å®¹")
                self.logger.info("   - å°è¯•å¤åˆ¶å•ä¸ªç£åŠ›é“¾æ¥")
            
            return torrents
            
        except Exception as e:
            self.stats['errors'] += 1
            self.logger.error(f"âŒ æŠ“å–æœç´¢é¡µé¢å¤±è´¥: {str(e)}")
            
            # æä¾›è¯¦ç»†çš„é”™è¯¯æŒ‡å¯¼
            if "ERR_CONNECTION_RESET" in str(e):
                self.logger.info("ğŸš¨ è¿æ¥è¢«é‡ç½®é”™è¯¯é€šå¸¸è¡¨ç¤º:")
                self.logger.info("   - ç½‘ç«™æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
                self.logger.info("   - IPå¯èƒ½è¢«ä¸´æ—¶é™åˆ¶")
                self.logger.info("   - éœ€è¦ä½¿ç”¨VPNæˆ–ä»£ç†")
            
            self.logger.info("ğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ:")
            self.logger.info("   1. æ‰‹åŠ¨è®¿é—®XXXClubæœç´¢é¡µé¢")
            self.logger.info("   2. å¤åˆ¶å•ä¸ªç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
            self.logger.info("   3. ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ ")
            
            raise CrawlerError(f"æŠ“å–æœç´¢é¡µé¢å¤±è´¥: {str(e)}") from e
    
    async def _parse_xxxclub_content(self, crawl_result, base_url: str) -> List[TorrentInfo]:
        """è§£æ crawl4ai æŠ“å–çš„å†…å®¹"""
        torrents = []
        
        try:
            # æš‚æ—¶è·³è¿‡LLMæå–ï¼Œç›´æ¥ä½¿ç”¨ç®€å•è§£ææ–¹æ³•
            # TODO: åç»­é‡æ–°å®ç°LLMæå–åŠŸèƒ½
            self.logger.info("ğŸ“ ä½¿ç”¨BeautifulSoupè§£æç½‘é¡µå†…å®¹")
            return await self._simple_parse_xxxclub(crawl_result.cleaned_html, base_url)
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æå†…å®¹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§£æ: {str(e)}")
            return await self._simple_parse_xxxclub(crawl_result.cleaned_html, base_url)
    
    async def _simple_parse_xxxclub(self, html_content: str, base_url: str) -> List[TorrentInfo]:
        """ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£ææ–¹æ³•"""
        torrents = []
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰liå…ƒç´ ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªæ ‡é¢˜è¡Œï¼‰
            list_items = soup.find_all('li')
            
            # è·³è¿‡ç¬¬ä¸€ä¸ªliï¼ˆæ ‡é¢˜è¡Œï¼‰
            if len(list_items) > 1:
                list_items = list_items[1:]
            
            self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(list_items)} ä¸ªç§å­é¡¹")
            
            for item in list_items:
                try:
                    # æŸ¥æ‰¾æ‰€æœ‰spanå…ƒç´ 
                    spans = item.find_all('span')
                    if len(spans) < 6:  # è‡³å°‘éœ€è¦6ä¸ªspan
                        continue
                    
                    # ç¬¬äºŒä¸ªspanåŒ…å«ç§å­è¯¦æƒ…é“¾æ¥
                    name_span = spans[1]
                    detail_link_elem = name_span.find('a', href=lambda x: x and '/torrents/details/' in x)
                    if not detail_link_elem:
                        continue
                    
                    detail_link = detail_link_elem['href']
                    title = detail_link_elem.get_text(strip=True)
                    
                    if not title:
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if detail_link.startswith('/'):
                        parsed_base = urllib.parse.urlparse(base_url)
                        detail_url = f"{parsed_base.scheme}://{parsed_base.netloc}{detail_link}"
                    else:
                        detail_url = detail_link
                    
                    # æå–å…¶ä»–ä¿¡æ¯
                    # ç¬¬å››ä¸ªspanåŒ…å«å¤§å°
                    size = spans[3].get_text(strip=True) if len(spans) > 3 else ""
                    
                    # ç¬¬äº”ä¸ªå’Œç¬¬å…­ä¸ªspanåŒ…å«seederså’Œleechers  
                    seeders = 0
                    if len(spans) > 4 and spans[4].get_text(strip=True).isdigit():
                        seeders = int(spans[4].get_text(strip=True))
                    
                    leechers = 0
                    if len(spans) > 5 and spans[5].get_text(strip=True).isdigit():
                        leechers = int(spans[5].get_text(strip=True))
                    
                    torrent_info = TorrentInfo(
                        title=title,
                        detail_url=detail_url,
                        size=size,
                        seeders=seeders,
                        leechers=leechers
                    )
                    
                    torrents.append(torrent_info)
                    self.logger.debug(f"âœ… è§£æç§å­: {title[:50]}... | å¤§å°: {size} | S/L: {seeders}/{leechers}")
                    
                except Exception as e:
                    self.logger.debug(f"è§£æç§å­é¡¹å¤±è´¥: {str(e)}")
                    continue
            
            return torrents
            
        except Exception as e:
            self.logger.error(f"âŒ ç®€å•è§£æå¤±è´¥: {str(e)}")
            return []
    
    async def extract_magnet_links(self, torrents: List[TorrentInfo]) -> List[TorrentInfo]:
        """ä»ç§å­è¯¦æƒ…é¡µé¢æå–ç£åŠ›é“¾æ¥"""
        self.logger.info(f"ğŸ”— å¼€å§‹æå– {len(torrents)} ä¸ªç§å­çš„ç£åŠ›é“¾æ¥")

        # å¦‚æœå¯ç”¨å¹¶å‘ä¸”ç§å­æ•°é‡è¾ƒå¤šï¼Œä½¿ç”¨å¹¶å‘æå–
        if (self.config.web_crawler.max_concurrent_extractions > 1 and
            len(torrents) > self.config.web_crawler.max_concurrent_extractions):
            return await self._extract_magnet_links_concurrent(torrents)
        else:
            return await self._extract_magnet_links_sequential(torrents)

    async def _extract_magnet_links_concurrent(self, torrents: List[TorrentInfo]) -> List[TorrentInfo]:
        """å¹¶å‘æå–ç£åŠ›é“¾æ¥"""
        self.logger.info(f"ğŸš€ ä½¿ç”¨å¹¶å‘æ¨¡å¼æå–ç£åŠ›é“¾æ¥ï¼Œæœ€å¤§å¹¶å‘æ•°: {self.config.web_crawler.max_concurrent_extractions}")

        # åˆ†æ‰¹å¤„ç†
        batch_size = self.config.web_crawler.max_concurrent_extractions
        results = []

        for i in range(0, len(torrents), batch_size):
            batch = torrents[i:i + batch_size]
            self.logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(torrents) + batch_size - 1)//batch_size}: {len(batch)} ä¸ªç§å­")

            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            tasks = []
            for torrent in batch:
                task = asyncio.create_task(self._extract_single_magnet_link(torrent))
                tasks.append(task)

            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ æå–å¤±è´¥: {batch[j].title[:50]}... - {str(result)}")
                    batch[j].status = "failed"
                    self.stats['errors'] += 1
                else:
                    batch[j] = result

            results.extend(batch)

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(torrents):
                delay = self.config.web_crawler.inter_request_delay * 2  # æ‰¹æ¬¡é—´å»¶è¿Ÿç¨é•¿
                await asyncio.sleep(delay)

        successful_extractions = len([t for t in results if t.status == "extracted"])
        self.logger.info(f"ğŸ¯ å¹¶å‘ç£åŠ›é“¾æ¥æå–å®Œæˆ: {successful_extractions}/{len(results)} æˆåŠŸ")

        return results

    async def _extract_single_magnet_link(self, torrent: TorrentInfo) -> TorrentInfo:
        """æå–å•ä¸ªç§å­çš„ç£åŠ›é“¾æ¥"""
        # å¢å¼ºå‹è¯¦æƒ…é¡µçˆ¬è™«é…ç½®ï¼Œä½¿ç”¨é…ç½®å‚æ•°
        crawler_config = {
            'verbose': False,
            'browser_type': 'chromium',
            'headless': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Referer': 'https://www.google.com/',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
            },
            'page_timeout': self.config.web_crawler.page_timeout,
            'wait_for': self.config.web_crawler.wait_for,
            'delay_before_return_html': self.config.web_crawler.delay_before_return,
            'proxy': self.config.web_crawler.proxy,
            'viewport': {'width': 1920, 'height': 1080},
            'timezone_id': 'Asia/Shanghai',
            'locale': 'zh-CN',
            'extra_http_headers': {
                'DNT': '1',
                'Sec-Ch-Ua': '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
            }
        }

        max_retries = self.config.web_crawler.max_retries
        base_delay = self.config.web_crawler.base_delay

        for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼Œé™åˆ¶æœ€å¤§å»¶è¿Ÿ
                        delay = min(base_delay * (2 ** (attempt - 1)) + random.uniform(0, 3),
                                  self.config.web_crawler.max_delay)
                        await asyncio.sleep(delay)

                    # åŠ¨æ€è°ƒæ•´çˆ¬è™«å‚æ•°
                    current_config = crawler_config.copy()
                    if attempt > 1:  # ç¬¬ä¸‰æ¬¡é‡è¯•åå¢åŠ è¶…æ—¶å’Œç­‰å¾…æ—¶é—´
                        current_config['page_timeout'] = min(current_config['page_timeout'] * 1.5, 180000)
                        current_config['wait_for'] = min(current_config['wait_for'] * 2, 15)
                        current_config['delay_before_return_html'] = min(current_config['delay_before_return_html'] * 2, 8)

                    # ä½¿ç”¨ç¼“å­˜è¯·æ±‚æ–¹æ³•
                    result = await self._make_request_with_cache(
                        url=torrent.detail_url,
                        wait_for=current_config['wait_for'],
                        page_timeout=current_config['page_timeout'],
                        bypass_cache=True,
                        delay_before_return_html=current_config['delay_before_return_html'],
                        css_selector="body",
                        js_code="window.scrollTo(0, document.body.scrollHeight/2);" if attempt > 0 else None,
                        extra_http_headers={
                            'Referer': torrent.detail_url if attempt > 0 else crawler_config['headers']['Referer']
                        }
                    )

                    if not result.success:
                        error_msg = str(result.error_message) if result.error_message else "Unknown error"
                        if attempt < max_retries - 1:
                            continue
                        else:
                            raise Exception(f"æŠ“å–å¤±è´¥: {error_msg}")

                    # æå–ç£åŠ›é“¾æ¥å¹¶è§£ææ–‡ä»¶å
                    magnet_link = await self._extract_magnet_from_content(result)

                    if magnet_link and validate_magnet_link(magnet_link):
                        torrent.magnet_link = magnet_link
                        torrent.status = "extracted"
                        self.stats['magnets_extracted'] += 1

                        # æ£€æŸ¥æ˜¯å¦é‡å¤å¹¶è·å–æ–‡ä»¶å
                        torrent_hash, torrent_name = parse_magnet(magnet_link)

                        # ä»…å½“ç£åŠ›é“¾æ¥åŒ…å«æ›´å®Œæ•´çš„æ–‡ä»¶åæ—¶æ‰è¦†ç›–
                        if torrent_name and len(torrent_name) > len(torrent.title):
                            torrent.title = torrent_name
                        if torrent_hash and torrent_hash in self.processed_hashes:
                            torrent.status = "duplicate"
                            self.stats['duplicates_skipped'] += 1
                        elif torrent_hash:
                            self.processed_hashes.add(torrent_hash)

                        return torrent
                    else:
                        if attempt < max_retries - 1:
                            continue
                        else:
                            torrent.status = "failed"
                            self.stats['errors'] += 1
                            return torrent

                except Exception as e:
                    if attempt < max_retries - 1:
                        continue
                    else:
                        torrent.status = "failed"
                        self.stats['errors'] += 1
                        raise e

        return torrent

    async def _extract_magnet_links_sequential(self, torrents: List[TorrentInfo]) -> List[TorrentInfo]:
        """é¡ºåºæå–ç£åŠ›é“¾æ¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        
        # å¢å¼ºå‹è¯¦æƒ…é¡µçˆ¬è™«é…ç½®ï¼Œä½¿ç”¨é…ç½®å‚æ•°
        crawler_config = {
            'verbose': False,
            'browser_type': 'chromium',
            'headless': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Referer': 'https://www.google.com/',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
            },
            'page_timeout': self.config.web_crawler.page_timeout,
            'wait_for': self.config.web_crawler.wait_for,
            'delay_before_return_html': self.config.web_crawler.delay_before_return,
            'proxy': self.config.web_crawler.proxy,
            'viewport': {'width': 1920, 'height': 1080},
            'timezone_id': 'Asia/Shanghai',
            'locale': 'zh-CN',
            'extra_http_headers': {
                'DNT': '1',
                'Sec-Ch-Ua': '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
            }
        }
        
        for i, torrent in enumerate(torrents, 1):
                max_retries = self.config.web_crawler.max_retries
                base_delay = self.config.web_crawler.base_delay
                last_error = None

                for attempt in range(max_retries):
                    try:
                        if attempt > 0:
                            # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼Œé™åˆ¶æœ€å¤§å»¶è¿Ÿ
                            delay = min(base_delay * (2 ** (attempt - 1)) + random.uniform(0, 5),
                                      self.config.web_crawler.max_delay)
                            self.logger.info(f"ğŸ”„ [{i}/{len(torrents)}] é‡è¯• {attempt+1}/{max_retries}, ç­‰å¾… {delay:.1f} ç§’: {torrent.title[:50]}...")
                            await asyncio.sleep(delay)
                        else:
                            self.logger.info(f"ğŸ” [{i}/{len(torrents)}] æå–ç£åŠ›é“¾æ¥: {torrent.title[:50]}...")

                        # åŠ¨æ€è°ƒæ•´çˆ¬è™«å‚æ•°
                        current_config = crawler_config.copy()
                        if attempt > 1:  # ç¬¬ä¸‰æ¬¡é‡è¯•åå¢åŠ è¶…æ—¶å’Œç­‰å¾…æ—¶é—´
                            current_config['page_timeout'] = min(current_config['page_timeout'] * 1.5, 180000)
                            current_config['wait_for'] = min(current_config['wait_for'] * 2, 15)
                            current_config['delay_before_return_html'] = min(current_config['delay_before_return_html'] * 2, 8)
                        
                        # ä½¿ç”¨ç¼“å­˜è¯·æ±‚æ–¹æ³•
                        result = await self._make_request_with_cache(
                            url=torrent.detail_url,
                            wait_for=current_config['wait_for'],
                            page_timeout=current_config['page_timeout'],
                            bypass_cache=True,
                            delay_before_return_html=current_config['delay_before_return_html'],
                            css_selector="body",
                            js_code="window.scrollTo(0, document.body.scrollHeight/2);" if attempt > 0 else None,
                            extra_http_headers={
                                'Referer': torrent.detail_url if attempt > 0 else crawler_config['headers']['Referer']
                            }
                        )
                        
                        if not result.success:
                            error_msg = str(result.error_message) if result.error_message else "Unknown error"
                            if "ERR_CONNECTION_RESET" in error_msg or "Connection reset" in error_msg:
                                self.logger.warning(f"âš ï¸ è¿æ¥è¢«é‡ç½®ï¼Œå°è¯• {attempt+1}/{max_retries}")
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(base_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                                    continue
                            else:
                                self.logger.warning(f"âš ï¸ æŠ“å–å¤±è´¥: {error_msg}")
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(base_delay)
                                    continue
                            break
                        
                        # æå–ç£åŠ›é“¾æ¥å¹¶è§£ææ–‡ä»¶å
                        magnet_link = await self._extract_magnet_from_content(result)
                        
                        if magnet_link and validate_magnet_link(magnet_link):
                            torrent.magnet_link = magnet_link
                            torrent.status = "extracted"
                            self.stats['magnets_extracted'] += 1
                            
                            # æ£€æŸ¥æ˜¯å¦é‡å¤å¹¶è·å–æ–‡ä»¶åï¼ˆä¿ç•™åŸå§‹æ ‡é¢˜ç”¨äºæ˜¾ç¤ºï¼‰
                            torrent_hash, torrent_name = parse_magnet(magnet_link)
                            self.logger.debug(f"ğŸ” ç£åŠ›é“¾æ¥è§£æ - åŸå§‹æ ‡é¢˜: {torrent.title} | è§£ææ–‡ä»¶å: {torrent_name}")
                            
                            # ä»…å½“ç£åŠ›é“¾æ¥åŒ…å«æ›´å®Œæ•´çš„æ–‡ä»¶åæ—¶æ‰è¦†ç›–
                            if torrent_name and len(torrent_name) > len(torrent.title):
                                torrent.title = torrent_name
                            if torrent_hash and torrent_hash in self.processed_hashes:
                                torrent.status = "duplicate"
                                self.stats['duplicates_skipped'] += 1
                                self.logger.info(f"âš ï¸ è·³è¿‡é‡å¤ç§å­: {torrent.title[:50]}...")
                            elif torrent_hash:
                                self.processed_hashes.add(torrent_hash)
                            
                            self.logger.info(f"âœ… æˆåŠŸæå–ç£åŠ›é“¾æ¥: {torrent.title[:30]}...")
                            break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        else:
                            if attempt < max_retries - 1:
                                retry_delay = base_delay * (attempt + 1)
                                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥ï¼Œé‡è¯• {attempt+1}/{max_retries}")
                                await asyncio.sleep(base_delay)
                                continue
                            else:
                                torrent.status = "failed"
                                self.stats['errors'] += 1
                                self.logger.warning(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆç£åŠ›é“¾æ¥: {torrent.title[:50]}...")
                        
                    except Exception as e:
                        error_msg = str(e)
                        if "ERR_CONNECTION_RESET" in error_msg or "Connection reset" in error_msg:
                            self.logger.warning(f"âš ï¸ è¿æ¥é‡ç½®é”™è¯¯ï¼Œå°è¯• {attempt+1}/{max_retries}: {torrent.title[:50]}...")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(base_delay * (attempt + 1))
                                continue
                        else:
                            self.logger.error(f"âŒ æå–å¤±è´¥: {torrent.title[:50]}... - {error_msg}")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(base_delay)
                                continue
                        
                        # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                        if attempt == max_retries - 1:
                            torrent.status = "failed"
                            self.stats['errors'] += 1
                            break
                
                # æ¯ä¸ªç§å­é—´çš„å»¶è¿Ÿï¼Œé¿å…è¢«æ£€æµ‹ä¸ºçˆ¬è™«ï¼Œä½¿ç”¨é…ç½®å‚æ•°
                if i < len(torrents):  # ä¸æ˜¯æœ€åä¸€ä¸ª
                    delay = self.config.web_crawler.inter_request_delay + random.uniform(0, 1)
                    await asyncio.sleep(delay)
        
        successful_extractions = len([t for t in torrents if t.status == "extracted"])
        self.logger.info(f"ğŸ¯ ç£åŠ›é“¾æ¥æå–å®Œæˆ: {successful_extractions}/{len(torrents)} æˆåŠŸ")
        
        return torrents
    
    async def _extract_magnet_from_content(self, crawl_result) -> Optional[str]:
        """ä»crawl4aiç»“æœä¸­æå–ç£åŠ›é“¾æ¥"""
        try:
            # åœ¨æ–‡æœ¬å†…å®¹ä¸­æœç´¢ç£åŠ›é“¾æ¥
            full_text = crawl_result.markdown + " " + crawl_result.cleaned_html
            
            # ç£åŠ›é“¾æ¥æ¨¡å¼
            magnet_patterns = [
                r'magnet:\?xt=urn:btih:[0-9a-fA-F]{40}[^"\s]*',
                r'magnet:\?xt=urn:btih:[0-9a-zA-Z]{32}[^"\s]*'
            ]
            
            for pattern in magnet_patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE)
                if matches:
                    magnet_link = matches[0].replace('&amp;', '&')
                    return magnet_link
            
            # å¦‚æœæ–‡æœ¬æœç´¢å¤±è´¥ï¼Œä½¿ç”¨HTMLè§£æ
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(crawl_result.cleaned_html, 'html.parser')
            
            # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            download_links = soup.find_all('a', href=True)
            for link in download_links:
                href = link['href']
                if href.startswith('magnet:'):
                    return href.replace('&amp;', '&')
            
            # æŸ¥æ‰¾æ–‡æœ¬åŒºåŸŸä¸­çš„ç£åŠ›é“¾æ¥
            text_areas = soup.find_all(['textarea', 'input'])
            for area in text_areas:
                text = area.get('value', '') or area.get_text()
                if text.startswith('magnet:'):
                    return text.replace('&amp;', '&')
            
            return None
            
        except Exception as e:
            self.logger.error(f"æå–ç£åŠ›é“¾æ¥å¤±è´¥: {str(e)}")
            return None
    
    async def add_torrents_to_qbittorrent(self, torrents: List[TorrentInfo]) -> List[TorrentInfo]:
        """
        å°†ç§å­æ‰¹é‡æ·»åŠ åˆ°qBittorrentå¹¶å¤„ç†åˆ†ç±»ã€é‡å‘½åç­‰

        Args:
            torrents: å¾…æ·»åŠ çš„ç§å­ä¿¡æ¯åˆ—è¡¨

        Returns:
            å¤„ç†åçš„ç§å­ä¿¡æ¯åˆ—è¡¨
        """
        if not torrents:
            return []

        self.logger.info(f"â• å¼€å§‹æ‰¹é‡æ·»åŠ  {len(torrents)} ä¸ªç§å­åˆ°qBittorrent...")

        for torrent in torrents:
            if not torrent.magnet_link or torrent.status != "extracted":
                continue

            try:
                # 1. AIåˆ†ç±»
                # ä½¿ç”¨é…ç½®ä¸­çš„AIåˆ†ç±»è®¾ç½®
                ai_classify_enabled = hasattr(self.config, 'ai_classify_torrents') and self.config.ai_classify_torrents
                if ai_classify_enabled and self.config.deepseek.api_key:
                    self.logger.info(f"ğŸ§  æ­£åœ¨ä¸º '{torrent.title}' è¿›è¡ŒAIåˆ†ç±»...")
                    category = await self.ai_classifier.classify(torrent.title, self.config.categories)
                    torrent.category = category
                    self.logger.info(f"   - AIåˆ†ç±»ç»“æœ: '{category}'")
                else:
                    torrent.category = "other"
                
                # 2. è·å–ä¿å­˜è·¯å¾„
                save_path = await self._get_category_save_path(torrent.category)

                # 3. æ·»åŠ ç§å­åˆ°qBittorrent (ä¸è¿›è¡Œé‡å‘½å)
                success = await self.qbt_client.add_torrent(
                    torrent.magnet_link,
                    torrent.category,
                    # ä½¿ç”¨é…ç½®ä¸­çš„æš‚åœè®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸ºFalse
                    paused=getattr(self.config, 'add_torrents_paused', False)
                )

                if success:
                    self.stats['torrents_added'] += 1
                    torrent.status = "added"
                    self.logger.info(f"âœ… æˆåŠŸæ·»åŠ ç§å­: {torrent.title}")
                else:
                    # å¯èƒ½æ˜¯å› ä¸ºå“ˆå¸Œå·²ç»å­˜åœ¨
                    if await self.qbt_client._is_duplicate(torrent.magnet_link):
                        self.stats['duplicates_skipped'] += 1
                        torrent.status = "duplicate"
                        self.logger.warning(f"âš ï¸ è·³è¿‡é‡å¤çš„ç§å­: {torrent.title}")
                    else:
                        self.stats['errors'] += 1
                        torrent.status = "failed"
                        self.logger.error(f"âŒ æ·»åŠ ç§å­å¤±è´¥: {torrent.title}")

            except Exception as e:
                torrent.status = "failed"
                self.stats['errors'] += 1
                self.logger.error(f"âŒ å¤„ç†ç§å­ '{torrent.title}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
                import traceback
                self.logger.debug(traceback.format_exc())

        # æ‰¹é‡å®Œæˆåå‘é€ä¸€æ¬¡é€šçŸ¥
        await self._notify_completion(torrents)
            
        return torrents
    
    async def _get_category_save_path(self, category: str) -> str:
        """è·å–åˆ†ç±»çš„ä¿å­˜è·¯å¾„"""
        try:
            existing_categories = await self.qbt_client.get_existing_categories()
            if category in existing_categories:
                return existing_categories[category].get('savePath', 'é»˜è®¤è·¯å¾„')
            elif category in self.config.categories:
                return self.config.categories[category].save_path
        except Exception:
            pass
        return "é»˜è®¤è·¯å¾„"
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'stats': self.stats.copy(),
            'processed_hashes_count': len(self.processed_hashes)
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'pages_crawled': 0,
            'torrents_found': 0,
            'magnets_extracted': 0,
            'torrents_added': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }
        self.processed_hashes.clear()
    
    async def _notify_completion(self, torrents: List[TorrentInfo]):
        """é€šçŸ¥æ‰¹é‡å¤„ç†å®Œæˆ"""
        added_count = len([t for t in torrents if t.status == "added"])
        failed_count = len([t for t in torrents if t.status == "failed"])
        duplicate_count = len([t for t in torrents if t.status == "duplicate"])
        
        if self.config.notifications.console.enabled:
            if self.notification_manager.use_colors:
                from colorama import Fore, Style
                print(f"\n{Fore.GREEN}âœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
                print(f"{Fore.CYAN}ğŸ“Š å¤„ç†ç»“æœ:")
                print(f"   æˆåŠŸæ·»åŠ : {Fore.GREEN}{added_count}")
                print(f"   å¤±è´¥æ•°é‡: {Fore.RED}{failed_count}")
                print(f"   é‡å¤è·³è¿‡: {Fore.YELLOW}{duplicate_count}")
                print(f"{Fore.GREEN}{'â”€'*50}{Style.RESET_ALL}")
            else:
                print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
                print(f"ğŸ“Š å¤„ç†ç»“æœ:")
                print(f"   æˆåŠŸæ·»åŠ : {added_count}")
                print(f"   å¤±è´¥æ•°é‡: {failed_count}")
                print(f"   é‡å¤è·³è¿‡: {duplicate_count}")
                print(f"{'â”€'*50}")

        # å‘é€é€šçŸ¥
        try:
            await self.notification_manager.send_statistics({
                'total_processed': len(torrents),
                'successful_adds': added_count,
                'failed_adds': failed_count,
                'duplicates_skipped': duplicate_count,
                'ai_classifications': 0,  # æš‚æ—¶è®¾ç½®ä¸º0
                'rule_classifications': 0,  # æš‚æ—¶è®¾ç½®ä¸º0
                'url_crawls': 1,
                'batch_adds': 1 if added_count > 0 else 0
            })
        except Exception as e:
            self.logger.warning(f"å‘é€å®Œæˆé€šçŸ¥å¤±è´¥: {str(e)}")

    async def extract_magnet_links_fallback(self, torrents: List[TorrentInfo]) -> List[TorrentInfo]:
        """
        å¤‡ç”¨æ–¹æ³•ï¼šå½“è¯¦æƒ…é¡µé¢æŠ“å–å¤±è´¥æ—¶çš„å¤„ç†æ–¹æ¡ˆ
        æš‚æ—¶æ ‡è®°æ‰€æœ‰ç§å­ä¸ºå¤±è´¥çŠ¶æ€ï¼Œå¹¶æä¾›ç”¨æˆ·æ‰‹åŠ¨æ“ä½œå»ºè®®
        """
        self.logger.warning("ğŸš¨ ç”±äºç½‘ç«™åçˆ¬è™«æœºåˆ¶ï¼Œæš‚æ—¶æ— æ³•è‡ªåŠ¨æå–ç£åŠ›é“¾æ¥")
        self.logger.info("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
        self.logger.info("   1. æ‰‹åŠ¨è®¿é—®ç§å­è¯¦æƒ…é¡µé¢")
        self.logger.info("   2. å¤åˆ¶ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
        self.logger.info("   3. ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ ")
        
        # æ ‡è®°æ‰€æœ‰ç§å­ä¸ºå¤±è´¥ï¼Œä½†ä¿ç•™ä¿¡æ¯ä¾›ç”¨æˆ·å‚è€ƒ
        for torrent in torrents:
            torrent.status = "failed"
            self.stats['errors'] += 1
            self.logger.info(f"ğŸ“„ ç§å­è¯¦æƒ…: {torrent.title[:60]}")
            self.logger.info(f"   URL: {torrent.detail_url}")
        
        return torrents


async def crawl_and_add_torrents(search_url: str, config: AppConfig, 
                                qbt_client: QBittorrentClient, 
                                max_pages: int = 1) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæŠ“å–å¹¶æ·»åŠ ç§å­
    
    Args:
        search_url: æœç´¢é¡µé¢URL
        config: åº”ç”¨é…ç½®
        qbt_client: qBittorrentå®¢æˆ·ç«¯
        max_pages: æœ€å¤§æŠ“å–é¡µæ•°
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    crawler = WebCrawler(config, qbt_client)
    
    try:
        # æŠ“å–æœç´¢é¡µé¢
        torrents = await crawler.crawl_xxxclub_search(search_url, max_pages)
        
        if not torrents:
            # è¯¦ç»†åˆ†æå¤±è´¥åŸå› å¹¶æä¾›æŒ‡å¯¼
            crawler.logger.info("ğŸ” åˆ†æå¯èƒ½çš„é—®é¢˜:")
            crawler.logger.info("   â¤ ç½‘ç«™å¯èƒ½æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
            crawler.logger.info("   â¤ æœç´¢å…³é”®è¯å¯èƒ½æ— ç»“æœ")
            crawler.logger.info("   â¤ ç½‘é¡µç»“æ„å¯èƒ½å‘ç”Ÿå˜åŒ–")
            
            crawler.logger.info("ğŸ› ï¸ å»ºè®®å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("   1. æ‰‹åŠ¨æ‰“å¼€æµè§ˆå™¨è®¿é—®æœç´¢é¡µé¢:")
            crawler.logger.info(f"      {search_url}")
            crawler.logger.info("   2. æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç æˆ–ç™»å½•")
            crawler.logger.info("   3. å¦‚æœèƒ½æ­£å¸¸è®¿é—®ï¼Œè¯·:")
            crawler.logger.info("      â€¢ æ‰¾åˆ°æ„Ÿå…´è¶£çš„ç§å­")
            crawler.logger.info("      â€¢ ç‚¹å‡»è¿›å…¥ç§å­è¯¦æƒ…é¡µé¢")
            crawler.logger.info("      â€¢ å¤åˆ¶ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
            crawler.logger.info("      â€¢ ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ ")
            crawler.logger.info("   4. å¦‚æœæœç´¢æ— ç»“æœï¼Œå°è¯•ä¿®æ”¹æœç´¢å…³é”®è¯")
            
            return {
                'success': False,
                'message': 'æœªæ‰¾åˆ°ä»»ä½•ç§å­ - å¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶æˆ–æœç´¢æ— ç»“æœ',
                'search_url': search_url,
                'suggestions': [
                    'æ‰‹åŠ¨è®¿é—®æœç´¢é¡µé¢ç¡®è®¤å†…å®¹',
                    'å¤åˆ¶å•ä¸ªç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿',
                    'æ£€æŸ¥æœç´¢å…³é”®è¯æ˜¯å¦æ­£ç¡®',
                    'ç¡®è®¤ç½‘ç«™æ˜¯å¦å¯æ­£å¸¸è®¿é—®'
                ],
                'stats': crawler.get_stats()
            }
        
        # æå–ç£åŠ›é“¾æ¥
        torrents = await crawler.extract_magnet_links(torrents)
        
        # æ£€æŸ¥æå–æˆåŠŸç‡ï¼Œå¦‚æœå¤ªä½åˆ™æä¾›å¤‡ç”¨æ–¹æ¡ˆ
        extracted_count = len([t for t in torrents if t.status == "extracted"])
        total_count = len(torrents)
        success_rate = extracted_count / total_count if total_count > 0 else 0
        
        if success_rate < 0.1 and total_count > 5:  # æˆåŠŸç‡ä½äº10%ä¸”ç§å­æ•°é‡å¤§äº5
            crawler.logger.warning(f"ğŸš¨ ç£åŠ›é“¾æ¥æå–æˆåŠŸç‡è¿‡ä½ ({success_rate:.1%})ï¼Œå¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶")
            crawler.logger.info("ğŸ’¡ ä¸ºæ‚¨æä¾›ç§å­è¯¦æƒ…é¡µé¢é“¾æ¥ï¼Œå¯æ‰‹åŠ¨å¤åˆ¶ç£åŠ›é“¾æ¥:")
            
            for i, torrent in enumerate(torrents[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                crawler.logger.info(f"[{i}] {torrent.title[:60]}")
                crawler.logger.info(f"    {torrent.detail_url}")
                if i < len(torrents):
                    crawler.logger.info("")
            
            if len(torrents) > 10:
                crawler.logger.info(f"... è¿˜æœ‰ {len(torrents) - 10} ä¸ªç§å­æœªæ˜¾ç¤º")
            
            return {
                'success': False,
                'message': f'æŠ“å–å—é˜»: æ‰¾åˆ°{total_count}ä¸ªç§å­ï¼Œä½†åªèƒ½æå–{extracted_count}ä¸ªç£åŠ›é“¾æ¥ã€‚è¯·æ‰‹åŠ¨è®¿é—®è¯¦æƒ…é¡µé¢å¤åˆ¶ç£åŠ›é“¾æ¥ã€‚',
                'torrents': [
                    {
                        'title': t.title,
                        'detail_url': t.detail_url,
                        'status': t.status,
                        'size': t.size
                    }
                    for t in torrents
                ],
                'stats': crawler.get_stats()
            }
        
        # æ·»åŠ åˆ°qBittorrent
        torrents = await crawler.add_torrents_to_qbittorrent(torrents)
        
        stats = crawler.get_stats()
        
        return {
            'success': True,
            'message': f"å¤„ç†å®Œæˆ: æ‰¾åˆ°{stats['stats']['torrents_found']}ä¸ªç§å­ï¼ŒæˆåŠŸæ·»åŠ {stats['stats']['torrents_added']}ä¸ª",
            'torrents': [
                {
                    'title': t.title,
                    'status': t.status,
                    'category': t.category,
                    'size': t.size
                }
                for t in torrents
            ],
            'stats': stats
        }
    except CrawlerError as e:
        # ä¸“é—¨å¤„ç†çˆ¬è™«é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„æŒ‡å¯¼
        error_msg = str(e)
        crawler.logger.error("ğŸš¨ ç½‘é¡µæŠ“å–é‡åˆ°é—®é¢˜")
        
        if "ERR_CONNECTION_RESET" in error_msg:
            crawler.logger.info("ğŸ” è¯Šæ–­: è¿æ¥è¢«é‡ç½®")
            crawler.logger.info("ğŸ“‹ è¿™é€šå¸¸è¡¨ç¤º:")
            crawler.logger.info("   â€¢ ç½‘ç«™æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®å¹¶ä¸»åŠ¨æ–­å¼€è¿æ¥")
            crawler.logger.info("   â€¢ IPåœ°å€å¯èƒ½è¢«ä¸´æ—¶é™åˆ¶")
            crawler.logger.info("   â€¢ ç½‘ç«™æœ‰è¾ƒå¼ºçš„åçˆ¬è™«ä¿æŠ¤")
            
            crawler.logger.info("ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("   1. ç«‹å³è§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("      â€¢ æ‰‹åŠ¨è®¿é—®æœç´¢é¡µé¢")
            crawler.logger.info("      â€¢ å¤åˆ¶ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
            crawler.logger.info("      â€¢ ç¨‹åºä¼šè‡ªåŠ¨æ·»åŠ ")
            crawler.logger.info("   2. é•¿æœŸè§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("      â€¢ ä½¿ç”¨VPNæ¢IP")
            crawler.logger.info("      â€¢ ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•")
            crawler.logger.info("      â€¢ é™ä½è®¿é—®é¢‘ç‡")
        else:
            crawler.logger.info("ğŸ” å…¶ä»–ç½‘ç»œé—®é¢˜")
            crawler.logger.info("ğŸ› ï¸ å»ºè®®:")
            crawler.logger.info("   â€¢ æ£€æŸ¥ç½‘ç»œè¿æ¥")
            crawler.logger.info("   â€¢ ç¡®è®¤ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
            crawler.logger.info("   â€¢ å°è¯•æ‰‹åŠ¨è®¿é—®é¡µé¢")
        
        return {
            'success': False,
            'message': f'ç½‘é¡µæŠ“å–å¤±è´¥: {error_msg}',
            'error_type': 'crawler_error',
            'search_url': search_url,
            'stats': crawler.get_stats()
        }
    except Exception as e:
        # å¤„ç†å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
        crawler.logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸé”™è¯¯: {str(e)}")
        return {
            'success': False,
            'message': f'å¤„ç†å¤±è´¥: {str(e)}',
            'error_type': 'unexpected_error',
            'stats': crawler.get_stats()
        }
    finally:
        # ç¡®ä¿æ¸…ç†WebCrawlerèµ„æº
        await crawler.cleanup()
