"""
å¢å¼ºçš„ç½‘é¡µçˆ¬è™« - é›†æˆæ‰€æœ‰å¥å£®æ€§åŠŸèƒ½

ç‰¹æ€§ï¼š
- é›†æˆç»Ÿä¸€å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- ä½¿ç”¨å¢å¼ºçš„ç¼“å­˜ç³»ç»Ÿ
- ä½¿ç”¨èµ„æºç®¡ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
- ä½¿ç”¨ç»Ÿä¸€ç†”æ–­å™¨å’Œé™æµ
- é›†æˆç›‘æ§å’ŒæŒ‡æ ‡
- æ™ºèƒ½é”™è¯¯æ¢å¤
- æ‰¹é‡æ“ä½œä¼˜åŒ–
"""

import asyncio
import logging
import time
import hashlib
import random
from typing import List, Dict, Optional, Set, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque

from crawl4ai import AsyncWebCrawler
from .exceptions_enhanced import retry, get_retry_config, RetryableError, NonRetryableError
from .enhanced_cache import get_global_cache
from .resource_manager import (
    BaseAsyncResource, AsyncResourcePool, managed_resource, get_global_tracker
)
from .concurrency import (
    AsyncThrottler, AsyncBatchProcessor, get_concurrency_config,
    async_throttle
)
from .monitoring import (
    get_metrics_collector, get_health_checker, PerformanceMonitor
)
from .circuit_breaker import (
    get_global_traffic_controller, UnifiedCircuitBreaker, UnifiedRateLimiter,
    CircuitBreakerConfig, RateLimitConfig, RateLimitStrategy
)

from .config import AppConfig
from .qbittorrent_client_enhanced import EnhancedQBittorrentClient
from .ai_classifier import AIClassifier
from .notifications import NotificationManager
from .utils import parse_magnet, validate_magnet_link
from .exceptions import CrawlerError

logger = logging.getLogger(__name__)


@dataclass
class TorrentInfo:
    """ç§å­ä¿¡æ¯æ•°æ®ç±»"""
    title: str
    detail_url: str
    magnet_link: str = ""
    size: str = ""
    seeders: int = 0
    leechers: int = 0
    category: str = ""
    status: str = "pending"


@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    url: str
    success: bool
    torrents: List[TorrentInfo]
    error: Optional[str] = None
    response_time: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)


class EnhancedWebCrawler(BaseAsyncResource):
    """
    å¢å¼ºçš„ç½‘é¡µçˆ¬è™«

    é›†æˆæ‰€æœ‰å¥å£®æ€§åŠŸèƒ½çš„ä¼ä¸šçº§çˆ¬è™«
    """

    def __init__(
        self,
        config: AppConfig,
        qbt_client: EnhancedQBittorrentClient
    ):
        super().__init__(f"web_crawler_{id(self)}")
        self.config = config
        self.qbt_client = qbt_client

        # åˆå§‹åŒ–å¢å¼ºæ¨¡å—
        self._init_enhanced_features()

        # çˆ¬è™«èµ„æºæ± 
        self._crawler_pool: Optional[AsyncResourcePool] = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_crawled': 0,
            'torrents_found': 0,
            'magnets_extracted': 0,
            'torrents_added': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }

        self.processed_hashes: Set[str] = set()

    def _init_enhanced_features(self):
        """åˆå§‹åŒ–å¢å¼ºåŠŸèƒ½"""
        # è·å–å…¨å±€ç»„ä»¶
        self._cache = get_global_cache()
        self._metrics = get_metrics_collector()
        self._health_checker = get_health_checker()
        self._traffic_controller = get_global_traffic_controller()
        self._tracker = get_global_tracker()
        self._performance_monitor = PerformanceMonitor(self._metrics)

        # åˆå§‹åŒ–ç»„ä»¶
        self.ai_classifier = AIClassifier(self.config.deepseek)
        self.notification_manager = NotificationManager(
            self.config.notifications.model_dump()
        )

        # æ³¨å†Œå¥åº·æ£€æŸ¥
        self._health_checker.register_check(
            f"web_crawler_{self.resource_id}",
            self._check_health,
            critical=False
        )

        # é…ç½®ç†”æ–­å™¨
        cb_config = CircuitBreakerConfig(
            failure_threshold=getattr(
                self.config.web_crawler,
                'circuit_breaker_threshold',
                5
            ),
            success_threshold=3,
            timeout=300.0,
            name=f"crawler_{self.resource_id}"
        )
        self._circuit_breaker = self._traffic_controller.add_circuit_breaker(
            f"crawler_{self.resource_id}",
            cb_config
        )

        # é…ç½®é™æµå™¨
        rl_config = RateLimitConfig(
            rate=getattr(
                self.config.web_crawler,
                'max_requests_per_minute',
                60
            ) / 60.0,
            strategy=RateLimitStrategy.TOKEN_BUCKET,
            name=f"crawler_{self.resource_id}"
        )
        self._rate_limiter = self._traffic_controller.add_rate_limiter(
            f"crawler_{self.resource_id}",
            rl_config
        )

        # é…ç½®èŠ‚æµå™¨
        concurrency_config = get_concurrency_config("medium")
        self._throttler = AsyncThrottler(concurrency_config['max_concurrent'])

        # é…ç½®æ‰¹å¤„ç†å™¨
        self._batch_processor = AsyncBatchProcessor(
            batch_size=5,
            max_wait_time=2.0,
            max_workers=concurrency_config['max_workers']
        )

    async def _do_close(self):
        """å…³é—­èµ„æº"""
        if self._crawler_pool:
            await self._crawler_pool.close()

        # æ¸…ç†AIåˆ†ç±»å™¨
        if hasattr(self.ai_classifier, 'cleanup'):
            await self.ai_classifier.cleanup()

        logger.info(f"WebCrawlerå·²å…³é—­: {self.resource_id}")

    async def _check_health(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self._crawler_pool:
                return {
                    "status": "warning",
                    "message": "Crawler pool not initialized"
                }

            pool_stats = self._crawler_pool.get_stats()
            return {
                "status": "healthy",
                "message": f"Pool size: {pool_stats['pool_size']}, "
                          f"Used: {pool_stats['used_size']}",
                "pool_stats": pool_stats
            }
        except Exception as e:
            return {
                "status": "critical",
                "message": f"Health check failed: {str(e)}"
            }

    async def _create_crawler(self) -> AsyncWebCrawler:
        """åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹"""
        crawler = AsyncWebCrawler(
            headless=True,
            browser_type="chromium",
            verbose=False,
            delay_before_return_html=2.0,
            js_code=[
                "window.scrollTo(0, document.body.scrollHeight);",
                "await new Promise(resolve => setTimeout(resolve, 1000));"
            ]
        )
        await crawler.start()

        # æ³¨å†Œåˆ°èµ„æºè·Ÿè¸ªå™¨
        await self._tracker.register_resource(
            resource_id=f"crawler_instance_{id(crawler)}",
            resource_type="web_crawler",
            resource=crawler,
            size_bytes=50 * 1024 * 1024,  # 50MBä¼°ç®—
            metadata={
                "created_at": datetime.now().isoformat()
            }
        )

        return crawler

    async def _initialize_pool(self):
        """åˆå§‹åŒ–çˆ¬è™«èµ„æºæ± """
        pool_size = getattr(
            self.config.web_crawler,
            'connection_pool_size',
            5
        )

        self._crawler_pool = AsyncResourcePool(
            create_func=self._create_crawler,
            max_size=pool_size,
            min_size=2,
            acquire_timeout=30.0,
            idle_timeout=300.0,
            resource_type="web_crawler"
        )

        await self._initialize_pool_base()

    async def _initialize_pool_base(self):
        """åˆå§‹åŒ–èµ„æºæ± åŸºç¡€"""
        await self._crawler_pool._initialize_pool()

    async def _cleanup_crawler(self, crawler: AsyncWebCrawler):
        """æ¸…ç†çˆ¬è™«å®ä¾‹"""
        if hasattr(crawler, 'close'):
            try:
                await crawler.close()
            except Exception as e:
                logger.warning(f"å…³é—­çˆ¬è™«å¤±è´¥: {str(e)}")

    async def _make_request_with_retry(
        self,
        url: str,
        **kwargs
    ) -> Any:
        """å¸¦é‡è¯•çš„è¯·æ±‚æ–¹æ³•"""
        retry_config = get_retry_config("network")

        for attempt in range(retry_config.max_attempts):
            try:
                return await self._traffic_controller.call(
                    self._do_crawl,
                    circuit_breaker_name=f"crawler_{self.resource_id}",
                    rate_limiter_name=f"crawler_{self.resource_id}",
                    url=url,
                    **kwargs
                )
            except RetryableError as e:
                if attempt < retry_config.max_attempts - 1:
                    delay = retry_config.get_delay(attempt + 1)
                    logger.warning(
                        f"çˆ¬å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_config.max_attempts}): "
                        f"{str(e)}ï¼Œ{delay:.2f}ç§’åé‡è¯•"
                    )
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"çˆ¬å–å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                    raise
            except NonRetryableError as e:
                logger.error(f"ä¸å¯é‡è¯•çš„é”™è¯¯: {str(e)}")
                raise
            except Exception as e:
                logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
                raise

    async def _do_crawl(self, url: str, **kwargs) -> Any:
        """å®é™…çš„çˆ¬å–å®ç°"""
        if not self._crawler_pool:
            await self._initialize_pool()

        # è·å–ç¼“å­˜é”®
        cache_key = f"crawler:{hashlib.md5(url.encode()).hexdigest()}"

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = await self._cache.get(cache_key)
        if cached_result is not None:
            logger.debug(f"ç¼“å­˜å‘½ä¸­: {url}")
            return cached_result

        start_time = time.time()

        # è·å–çˆ¬è™«å®ä¾‹
        async with managed_resource(
            create_func=lambda: self._crawler_pool.acquire(),
            resource_id=f"crawl_{id(url)}",
            pool=self._crawler_pool
        ) as crawler:
            # æ‰§è¡Œçˆ¬å–
            result = await crawler.arun(url=url, **kwargs)

            response_time = (time.time() - start_time) * 1000

            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            await self._performance_monitor.track_request(
                endpoint="crawl",
                duration_ms=response_time,
                success=True
            )

            # ç¼“å­˜ç»“æœ
            await self._cache.set(
                cache_key,
                result,
                ttl=getattr(
                    self.config.web_crawler,
                    'cache_ttl_seconds',
                    3600
                )
            )

            return result

    async def crawl_xxxclub_search(
        self,
        search_url: str,
        max_pages: int = 1
    ) -> List[TorrentInfo]:
        """
        æŠ“å–XXXClubæœç´¢é¡µé¢
        """
        logger.info(f"ğŸ•·ï¸ å¼€å§‹æŠ“å–XXXClubæœç´¢é¡µé¢: {search_url}")

        torrents = []

        # å¢å¼ºçš„çˆ¬è™«é…ç½®
        crawler_config = self._get_crawler_config()

        try:
            for page in range(1, max_pages + 1):
                # æ„å»ºåˆ†é¡µURL
                if page > 1:
                    if '?' in search_url:
                        page_url = f"{search_url}&page={page}"
                    else:
                        page_url = f"{search_url}?page={page}"
                else:
                    page_url = search_url

                logger.info(f"æŠ“å–ç¬¬ {page}/{max_pages} é¡µ: {page_url}")

                # ä½¿ç”¨æ‰¹å¤„ç†å™¨
                page_torrents = await self._batch_processor.process(
                    self._crawl_page,
                    page_url,
                    crawler_config
                )

                if page_torrents:
                    torrents.extend(page_torrents)
                    logger.info(f"ç¬¬ {page} é¡µæ‰¾åˆ° {len(page_torrents)} ä¸ªç§å­")
                else:
                    logger.warning(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°ç§å­")

                # é¡µé¢é—´éš”
                if page < max_pages:
                    await asyncio.sleep(random.uniform(1, 3))

        except Exception as e:
            logger.error(f"æŠ“å–å¤±è´¥: {str(e)}")
            self.stats['errors'] += 1
            raise CrawlerError(f"æŠ“å–XXXClubå¤±è´¥: {str(e)}") from e

        # æ›´æ–°ç»Ÿè®¡
        self.stats['pages_crawled'] += max_pages
        self.stats['torrents_found'] += len(torrents)

        # æå–ç£åŠ›é“¾æ¥
        extracted = await self._extract_magnets_from_torrents(torrents)
        self.stats['magnets_extracted'] += len(extracted)

        logger.info(
            f"æŠ“å–å®Œæˆ: æ‰¾åˆ° {len(torrents)} ä¸ªç§å­ï¼Œ"
            f"æå– {len(extracted)} ä¸ªç£åŠ›é“¾æ¥"
        )

        return extracted

    def _get_crawler_config(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«é…ç½®"""
        return {
            'verbose': False,
            'browser_type': 'chromium',
            'headless': True,
            'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                         'AppleWebKit/537.36 (KHTML, like Gecko) '
                         'Chrome/125.0.0.0 Safari/537.36',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,'
                         'image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
            },
            'page_timeout': getattr(
                self.config.web_crawler,
                'page_timeout',
                30000
            ),
            'wait_for': getattr(
                self.config.web_crawler,
                'wait_for',
                "css:.torrent-list"
            ),
            'delay_before_return_html': getattr(
                self.config.web_crawler,
                'delay_before_return',
                2.0
            ),
            'proxy': getattr(
                self.config.web_crawler,
                'proxy',
                None
            ),
            'viewport': {'width': 1920, 'height': 1080},
            'timezone_id': 'Asia/Shanghai',
            'locale': 'zh-CN',
        }

    async def _crawl_page(self, page_url: str, config: Dict[str, Any]) -> List[TorrentInfo]:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        try:
            result = await self._make_request_with_retry(page_url, **config)

            if not result.success:
                logger.warning(f"é¡µé¢çˆ¬å–å¤±è´¥: {page_url}")
                return []

            # è§£æé¡µé¢å†…å®¹
            torrents = self._parse_torrent_list(result.html, page_url)

            # è¿‡æ»¤é‡å¤
            unique_torrents = []
            for torrent in torrents:
                if torrent.magnet_link:
                    torrent_hash, _ = parse_magnet(torrent.magnet_link)
                    if torrent_hash and torrent_hash not in self.processed_hashes:
                        self.processed_hashes.add(torrent_hash)
                        unique_torrents.append(torrent)

            return unique_torrents

        except Exception as e:
            logger.error(f"çˆ¬å–é¡µé¢å¤±è´¥ {page_url}: {str(e)}")
            return []

    def _parse_torrent_list(self, html: str, url: str) -> List[TorrentInfo]:
        """è§£æç§å­åˆ—è¡¨"""
        # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„è§£æé€»è¾‘
        # ç”±äºéœ€è¦ BeautifulSoup æˆ–å…¶ä»–è§£æåº“ï¼Œè¿™é‡Œæä¾›æ¡†æ¶
        torrents = []

        # ç¤ºä¾‹è§£æé€»è¾‘
        import re

        # æŸ¥æ‰¾ç£åŠ›é“¾æ¥
        magnet_pattern = r'magnet:\?xt=urn:btih:[a-fA-F0-9]{32,40}'
        magnets = re.findall(magnet_pattern, html)

        # æŸ¥æ‰¾ç§å­ä¿¡æ¯
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®Œå–„

        return torrents

    async def _extract_magnets_from_torrents(
        self,
        torrents: List[TorrentInfo]
    ) -> List[TorrentInfo]:
        """ä»ç§å­åˆ—è¡¨ä¸­æå–ç£åŠ›é“¾æ¥"""
        extracted = []

        for torrent in torrents:
            if torrent.magnet_link and validate_magnet_link(torrent.magnet_link):
                # ä½¿ç”¨AIåˆ†ç±»å™¨åˆ†ç±»
                try:
                    category = await self.ai_classifier.classify_content(
                        torrent.title
                    )
                    torrent.category = category
                except Exception as e:
                    logger.warning(f"åˆ†ç±»å¤±è´¥: {str(e)}")
                    torrent.category = "æœªåˆ†ç±»"

                extracted.append(torrent)

        return extracted

    async def crawl_batch(
        self,
        urls: List[str],
        max_concurrent: int = 5
    ) -> List[CrawlResult]:
        """æ‰¹é‡çˆ¬å–"""
        logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªURL")

        results = []

        # ä½¿ç”¨èŠ‚æµå™¨æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(max_concurrent)

        async def crawl_single(url: str) -> CrawlResult:
            async with semaphore:
                start_time = time.time()
                try:
                    result = await self._make_request_with_retry(url)
                    response_time = (time.time() - start_time) * 1000

                    return CrawlResult(
                        url=url,
                        success=True,
                        torrents=[],
                        response_time=response_time
                    )
                except Exception as e:
                    response_time = (time.time() - start_time) * 1000
                    return CrawlResult(
                        url=url,
                        success=False,
                        torrents=[],
                        error=str(e),
                        response_time=response_time
                    )

        # å¹¶å‘æ‰§è¡Œ
        tasks = [crawl_single(url) for url in urls]
        results = await asyncio.gather(*tasks)

        # ç»Ÿè®¡
        success_count = sum(1 for r in results if r.success)
        logger.info(
            f"æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}/{len(urls)}"
        )

        return results

    async def process_and_add_torrents(
        self,
        torrents: List[TorrentInfo]
    ) -> Dict[str, int]:
        """å¤„ç†å¹¶æ·»åŠ ç§å­åˆ°qBittorrent"""
        logger.info(f"å¼€å§‹å¤„ç† {len(torrents)} ä¸ªç§å­")

        results = {
            'added': 0,
            'skipped': 0,
            'failed': 0
        }

        # æŒ‰åˆ†ç±»åˆ†ç»„
        categories = defaultdict(list)
        for torrent in torrents:
            categories[torrent.category].append(torrent)

        # å¤„ç†æ¯ä¸ªåˆ†ç±»
        for category, category_torrents in categories.items():
            logger.info(f"å¤„ç†åˆ†ç±»: {category} ({len(category_torrents)} ä¸ªç§å­)")

            # è½¬æ¢ä¸º(magnet, category)æ ¼å¼
            magnet_pairs = [
                (torrent.magnet_link, category)
                for torrent in category_torrents
            ]

            try:
                # ä½¿ç”¨æ‰¹é‡æ·»åŠ 
                batch_result = await self.qbt_client.add_torrents_batch(
                    magnet_pairs,
                    batch_size=10
                )

                results['added'] += batch_result['success_count']
                results['skipped'] += batch_result['skipped_count']
                results['failed'] += batch_result['failed_count']

            except Exception as e:
                logger.error(f"æ‰¹é‡æ·»åŠ å¤±è´¥: {str(e)}")
                results['failed'] += len(category_torrents)

        # æ›´æ–°ç»Ÿè®¡
        self.stats['torrents_added'] += results['added']
        self.stats['duplicates_skipped'] += results['skipped']

        logger.info(
            f"å¤„ç†å®Œæˆ: æ·»åŠ  {results['added']}, "
            f"è·³è¿‡ {results['skipped']}, å¤±è´¥ {results['failed']}"
        )

        return results

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        circuit_stats = self._circuit_breaker.get_stats()
        rate_limiter_stats = self._rate_limiter.get_stats()

        return {
            "resource_id": self.resource_id,
            "stats": self.stats.copy(),
            "circuit_breaker": circuit_stats,
            "rate_limiter": rate_limiter_stats,
            "throttler": {
                "active_tasks": self._throttler.get_stats().active_tasks,
                "queue_size": self._throttler.get_stats().queue_size
            },
            "processed_hashes": len(self.processed_hashes)
        }

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._initialize_pool()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.close()
        return False
