#!/usr/bin/env python3
"""
æ€§èƒ½å¯¹æ¯”è„šæœ¬ï¼šå¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚

è¿™ä¸ªè„šæœ¬ç”¨äºå¯¹æ¯”ï¼š
1. ç£åŠ›é“¾æ¥æå–é€Ÿåº¦
2. é…ç½®å‚æ•°å¯¹æ€§èƒ½çš„å½±å“
3. å¹¶å‘vsé¡ºåºå¤„ç†çš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import time
import logging
from pathlib import Path
import sys
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from qbittorrent_monitor.config import ConfigManager, WebCrawlerConfig
from qbittorrent_monitor.utils import parse_magnet, validate_magnet_link

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('PerformanceComparison')


class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.results = {}
    
    def start_timer(self, name: str):
        """å¼€å§‹è®¡æ—¶"""
        self.results[name] = {'start': time.time()}
    
    def end_timer(self, name: str):
        """ç»“æŸè®¡æ—¶"""
        if name in self.results:
            self.results[name]['end'] = time.time()
            self.results[name]['duration'] = self.results[name]['end'] - self.results[name]['start']
    
    def get_duration(self, name: str) -> float:
        """è·å–è€—æ—¶"""
        return self.results.get(name, {}).get('duration', 0)
    
    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        logger.info("ğŸ“Š æ€§èƒ½åˆ†ææ‘˜è¦:")
        for name, data in self.results.items():
            if 'duration' in data:
                logger.info(f"   - {name}: {data['duration']:.3f}s")


async def simulate_old_config_extraction(num_torrents: int = 10):
    """æ¨¡æ‹Ÿæ—§é…ç½®ä¸‹çš„æå–è¿‡ç¨‹"""
    logger.info(f"ğŸŒ æ¨¡æ‹Ÿæ—§é…ç½®æå– {num_torrents} ä¸ªç§å­...")

    # æ—§é…ç½®å‚æ•°ï¼ˆç¡¬ç¼–ç çš„æ…¢é€Ÿå‚æ•°ï¼‰
    old_config = {
        'page_timeout': 120000,  # 120ç§’
        'wait_for': 10,          # 10ç§’ç­‰å¾…
        'delay_before_return': 5, # 5ç§’å»¶è¿Ÿ
        'max_retries': 5,        # 5æ¬¡é‡è¯•
        'base_delay': 15,        # 15ç§’åŸºç¡€å»¶è¿Ÿ
        'inter_request_delay': 3, # 3ç§’è¯·æ±‚é—´å»¶è¿Ÿ
    }

    total_time = 0

    for i in range(num_torrents):
        # æ¨¡æ‹Ÿå•ä¸ªç§å­æå–æ—¶é—´ï¼ˆé¡ºåºå¤„ç†ï¼‰
        extraction_time = (
            old_config['wait_for'] +           # é¡µé¢ç­‰å¾…æ—¶é—´
            old_config['delay_before_return']  # è¿”å›å‰å»¶è¿Ÿ
        )

        # è¯·æ±‚é—´å»¶è¿Ÿï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
        if i < num_torrents - 1:
            extraction_time += old_config['inter_request_delay']

        await asyncio.sleep(extraction_time)
        total_time += extraction_time

        if (i + 1) % 5 == 0:
            logger.info(f"   å·²å¤„ç† {i + 1}/{num_torrents} ä¸ªç§å­")

    logger.info(f"ğŸŒ æ—§é…ç½®æå–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s")
    return total_time


async def simulate_new_config_extraction(num_torrents: int = 10):
    """æ¨¡æ‹Ÿæ–°é…ç½®ä¸‹çš„æå–è¿‡ç¨‹"""
    logger.info(f"ğŸš€ æ¨¡æ‹Ÿæ–°é…ç½®æå– {num_torrents} ä¸ªç§å­...")

    # æ–°é…ç½®å‚æ•°ï¼ˆä¼˜åŒ–åçš„å¿«é€Ÿå‚æ•°ï¼‰
    new_config = {
        'page_timeout': 60000,   # 60ç§’
        'wait_for': 3,           # 3ç§’ç­‰å¾…
        'delay_before_return': 2, # 2ç§’å»¶è¿Ÿ
        'max_retries': 3,        # 3æ¬¡é‡è¯•
        'base_delay': 5,         # 5ç§’åŸºç¡€å»¶è¿Ÿ
        'inter_request_delay': 1.5, # 1.5ç§’è¯·æ±‚é—´å»¶è¿Ÿ
        'max_concurrent': 3,     # æœ€å¤§å¹¶å‘æ•°
    }

    # æ¨¡æ‹Ÿå¹¶å‘å¤„ç†
    batch_size = new_config['max_concurrent']
    total_time = 0

    for i in range(0, num_torrents, batch_size):
        batch_end = min(i + batch_size, num_torrents)
        batch_size_actual = batch_end - i

        # æ¨¡æ‹Ÿå¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡ - å¹¶å‘æ‰§è¡Œï¼Œæ‰€ä»¥æ—¶é—´ä¸ç´¯åŠ 
        extraction_time = (
            new_config['wait_for'] +           # é¡µé¢ç­‰å¾…æ—¶é—´
            new_config['delay_before_return']  # è¿”å›å‰å»¶è¿Ÿ
        )

        await asyncio.sleep(extraction_time)
        total_time += extraction_time  # å¹¶å‘å¤„ç†ï¼Œæ—¶é—´ä¸ä¹˜ä»¥æ‰¹æ¬¡å¤§å°

        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        if batch_end < num_torrents:
            inter_delay = new_config['inter_request_delay']
            await asyncio.sleep(inter_delay)
            total_time += inter_delay

        logger.info(f"   å·²å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}, ç§å­ {batch_end}/{num_torrents}")

    logger.info(f"ğŸš€ æ–°é…ç½®æå–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s")
    return total_time


def test_magnet_parsing_performance():
    """æµ‹è¯•ç£åŠ›é“¾æ¥è§£ææ€§èƒ½"""
    logger.info("âš¡ æµ‹è¯•ç£åŠ›é“¾æ¥è§£ææ€§èƒ½...")
    
    test_magnets = [
        "magnet:?xt=urn:btih:1234567890abcdef1234567890abcdef12345678&dn=Test+Movie+2024+1080p+BluRay&tr=udp://tracker.example.com:80",
        "magnet:?xt=urn:btih:abcdef1234567890abcdef1234567890abcdef12&dn=Test+TV+Show+S01E01&tr=udp://tracker.example.com:80",
        "magnet:?xt=urn:btih:fedcba0987654321fedcba0987654321fedcba09&dn=Test+Game+PC&tr=udp://tracker.example.com:80",
    ]
    
    # æµ‹è¯•è§£ææ€§èƒ½
    iterations = 10000
    start_time = time.time()
    
    for i in range(iterations):
        for magnet in test_magnets:
            hash_val, name = parse_magnet(magnet)
            is_valid = validate_magnet_link(magnet)
    
    end_time = time.time()
    duration = end_time - start_time
    total_operations = iterations * len(test_magnets)
    
    logger.info(f"âœ… ç£åŠ›é“¾æ¥è§£ææ€§èƒ½æµ‹è¯•å®Œæˆ")
    logger.info(f"   - æ€»æ“ä½œæ•°: {total_operations}")
    logger.info(f"   - æ€»è€—æ—¶: {duration:.3f}s")
    logger.info(f"   - å¹³å‡è€—æ—¶: {duration/total_operations*1000:.3f}ms/æ¬¡")
    logger.info(f"   - å¤„ç†é€Ÿåº¦: {total_operations/duration:.0f} æ¬¡/ç§’")
    
    return duration


async def compare_extraction_performance():
    """å¯¹æ¯”æå–æ€§èƒ½"""
    logger.info("ğŸ å¼€å§‹æå–æ€§èƒ½å¯¹æ¯”...")
    
    profiler = PerformanceProfiler()
    num_torrents = 20  # æµ‹è¯•20ä¸ªç§å­
    
    # æµ‹è¯•æ—§é…ç½®
    profiler.start_timer("old_config")
    old_time = await simulate_old_config_extraction(num_torrents)
    profiler.end_timer("old_config")
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(1)
    
    # æµ‹è¯•æ–°é…ç½®
    profiler.start_timer("new_config")
    new_time = await simulate_new_config_extraction(num_torrents)
    profiler.end_timer("new_config")
    
    # è®¡ç®—æ€§èƒ½æå‡
    speedup = old_time / new_time if new_time > 0 else 0
    time_saved = old_time - new_time
    efficiency_gain = (time_saved / old_time) * 100 if old_time > 0 else 0
    
    logger.info("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    logger.info(f"   - ç§å­æ•°é‡: {num_torrents}")
    logger.info(f"   - æ—§é…ç½®è€—æ—¶: {old_time:.3f}s")
    logger.info(f"   - æ–°é…ç½®è€—æ—¶: {new_time:.3f}s")
    logger.info(f"   - æ€§èƒ½æå‡: {speedup:.2f}x")
    logger.info(f"   - èŠ‚çœæ—¶é—´: {time_saved:.3f}s")
    logger.info(f"   - æ•ˆç‡æå‡: {efficiency_gain:.1f}%")
    
    return {
        'old_time': old_time,
        'new_time': new_time,
        'speedup': speedup,
        'time_saved': time_saved,
        'efficiency_gain': efficiency_gain
    }


async def test_config_loading_performance():
    """æµ‹è¯•é…ç½®åŠ è½½æ€§èƒ½"""
    logger.info("âš™ï¸ æµ‹è¯•é…ç½®åŠ è½½æ€§èƒ½...")
    
    iterations = 100
    start_time = time.time()
    
    for i in range(iterations):
        config_manager = ConfigManager()
        config = await config_manager.load_config()
        
        # éªŒè¯æ–°é…ç½®é¡¹
        assert hasattr(config, 'web_crawler')
        assert hasattr(config.web_crawler, 'max_concurrent_extractions')
    
    end_time = time.time()
    duration = end_time - start_time
    
    logger.info(f"âœ… é…ç½®åŠ è½½æ€§èƒ½æµ‹è¯•å®Œæˆ")
    logger.info(f"   - åŠ è½½æ¬¡æ•°: {iterations}")
    logger.info(f"   - æ€»è€—æ—¶: {duration:.3f}s")
    logger.info(f"   - å¹³å‡è€—æ—¶: {duration/iterations*1000:.3f}ms/æ¬¡")
    
    return duration


def generate_performance_report(results: dict):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    logger.info("ğŸ“‹ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
    
    report = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'summary': {
            'extraction_speedup': f"{results['speedup']:.2f}x",
            'time_saved': f"{results['time_saved']:.3f}s",
            'efficiency_gain': f"{results['efficiency_gain']:.1f}%"
        },
        'details': results,
        'recommendations': [
            "ä½¿ç”¨æ–°çš„é…ç½®å‚æ•°å¯ä»¥æ˜¾è‘—æå‡ç£åŠ›é“¾æ¥æå–é€Ÿåº¦",
            "å¹¶å‘å¤„ç†èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ€»ä½“å¤„ç†æ—¶é—´",
            "ä¼˜åŒ–åçš„é‡è¯•ç­–ç•¥å‡å°‘äº†ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´",
            "å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”ç”¨è¿™äº›ä¼˜åŒ–é…ç½®"
        ]
    }
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = Path('performance_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return report


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
    
    try:
        # 1. æµ‹è¯•ç£åŠ›é“¾æ¥è§£ææ€§èƒ½
        parsing_time = test_magnet_parsing_performance()
        
        # 2. æµ‹è¯•é…ç½®åŠ è½½æ€§èƒ½
        config_time = await test_config_loading_performance()
        
        # 3. å¯¹æ¯”æå–æ€§èƒ½
        extraction_results = await compare_extraction_performance()
        
        # 4. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        all_results = {
            **extraction_results,
            'parsing_time': parsing_time,
            'config_loading_time': config_time
        }
        
        report = generate_performance_report(all_results)
        
        logger.info("ğŸ‰ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
        logger.info("ğŸ“Š ä¸»è¦æ”¹è¿›:")
        logger.info(f"   - æå–é€Ÿåº¦æå‡: {extraction_results['speedup']:.2f}x")
        logger.info(f"   - æ•ˆç‡æå‡: {extraction_results['efficiency_gain']:.1f}%")
        logger.info(f"   - æ¯20ä¸ªç§å­èŠ‚çœ: {extraction_results['time_saved']:.1f}ç§’")
        
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
