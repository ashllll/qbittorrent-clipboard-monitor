"""
åŸºäº crawl4ai çš„ç½‘é¡µçˆ¬è™«æ¨¡å—

ä½¿ç”¨ä¸“ä¸šçš„ crawl4ai åº“è¿›è¡Œç½‘ç«™æŠ“å–å’Œå†…å®¹æå–
"""

import asyncio
import logging
import random
import re
import urllib.parse
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

from crawl4ai import AsyncWebCrawler, LLMConfig
from crawl4ai.extraction_strategy import CosineStrategy, LLMExtractionStrategy

from .ai_classifier import AIClassifier
from .config import AppConfig
from .exceptions import CrawlerError
from .qbittorrent_client import QBittorrentClient
from .utils import NotificationManager, parse_magnet, validate_magnet_link


@dataclass
class TorrentInfo:
    """ç§å­ä¿¡æ¯æ•°æ®ç±»"""

    title: str
    detail_url: str
    magnet_link: str = ""
    size: str = ""
    seeders: int = 0
    leechers: int = 0
    category: str = ""
    status: str = "pending"  # pending, extracted, added, failed, duplicate


class WebCrawler:
    """åŸºäº crawl4ai çš„ç½‘é¡µçˆ¬è™«ç±»"""

    def __init__(self, config: AppConfig, qbt_client: QBittorrentClient):
        self.config = config
        self.qbt_client = qbt_client
        self.logger = logging.getLogger("WebCrawler")

        # åˆå§‹åŒ–ç»„ä»¶
        self.ai_classifier = AIClassifier(config.deepseek)
        self.notification_manager = NotificationManager(
            config.notifications.model_dump()
        )

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "pages_crawled": 0,
            "torrents_found": 0,
            "magnets_extracted": 0,
            "torrents_added": 0,
            "duplicates_skipped": 0,
            "errors": 0,
        }

        self.processed_hashes: Set[str] = set()

    async def crawl_xxxclub_search(
        self, search_url: str, max_pages: int = 1
    ) -> List[TorrentInfo]:
        """
        æŠ“å–XXXClubæœç´¢é¡µé¢

        Args:
            search_url: æœç´¢é¡µé¢URL
            max_pages: æœ€å¤§æŠ“å–é¡µæ•°

        Returns:
            ç§å­ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info(f"ğŸ•·ï¸ å¼€å§‹æŠ“å–XXXClubæœç´¢é¡µé¢: {search_url}")
        torrents = []

        # å¢å¼ºçš„çˆ¬è™«é…ç½®ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
        crawler_config = {
            "verbose": False,
            "browser_type": "chromium",
            "headless": True,
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
            "headers": {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
                "Referer": "https://www.google.com/",
            },
            "page_timeout": 90000,  # 90ç§’è¶…æ—¶
            "wait_for": 5,  # ç­‰å¾…5ç§’
            "delay_before_return_html": 3,  # ç­‰å¾…3ç§’å†è¿”å›
            "proxy": self.config.proxy if hasattr(self.config, "proxy") else None,
            "viewport": {"width": 1920, "height": 1080},
            "timezone_id": "Asia/Shanghai",
            "locale": "zh-CN",
            "geolocation": {"latitude": 39.9042, "longitude": 116.4074},  # åŒ—äº¬åæ ‡
            "permissions": ["geolocation"],
            "extra_http_headers": {
                "DNT": "1",
                "Sec-Ch-Ua": '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
                "Sec-Ch-Ua-Mobile": "?0",
                "Sec-Ch-Ua-Platform": '"Windows"',
            },
        }

        try:
            async with AsyncWebCrawler(**crawler_config) as crawler:
                for page in range(1, max_pages + 1):
                    # æ„å»ºåˆ†é¡µURL
                    if page > 1:
                        if "?" in search_url:
                            page_url = f"{search_url}&page={page}"
                        else:
                            page_url = f"{search_url}?page={page}"
                    else:
                        page_url = search_url

                    self.logger.info(f"ğŸ“„ æŠ“å–ç¬¬ {page} é¡µ: {page_url}")

                    # æ™ºèƒ½é‡è¯•æœºåˆ¶
                    max_retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°å¢åŠ åˆ°5æ¬¡
                    base_delay = 10  # åŸºç¡€å»¶è¿Ÿ10ç§’
                    success = False
                    last_error = None

                    for attempt in range(max_retries):
                        try:
                            if attempt > 0:
                                # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                                delay = min(
                                    base_delay * (2 ** (attempt - 1))
                                    + random.uniform(0, 5),
                                    300,
                                )  # æœ€å¤§ä¸è¶…è¿‡5åˆ†é’Ÿ
                                self.logger.info(
                                    f"ğŸ”„ ç¬¬ {page} é¡µé‡è¯• {attempt+1}/{max_retries}, ç­‰å¾… {delay:.1f} ç§’"
                                )
                                await asyncio.sleep(delay)

                            # åŠ¨æ€è°ƒæ•´çˆ¬è™«å‚æ•°
                            current_config = crawler_config.copy()
                            if attempt > 1:  # ç¬¬äºŒæ¬¡é‡è¯•åå¢åŠ ç­‰å¾…æ—¶é—´
                                current_config["wait_for"] = 10
                                current_config["delay_before_return_html"] = 5

                            # ä½¿ç”¨ crawl4ai æŠ“å–é¡µé¢
                            result = await crawler.arun(
                                url=page_url,
                                wait_for=current_config["wait_for"],
                                js_code=(
                                    None
                                    if attempt < 2
                                    else "window.scrollTo(0, document.body.scrollHeight);"
                                ),  # ç¬¬ä¸‰æ¬¡é‡è¯•åæ¨¡æ‹Ÿæ»šåŠ¨
                                css_selector="ul.tsearch li",
                                bypass_cache=True,
                                page_timeout=current_config["page_timeout"],
                                delay_before_return_html=current_config[
                                    "delay_before_return_html"
                                ],
                                extra_http_headers={
                                    "X-Requested-With": (
                                        "XMLHttpRequest" if attempt > 1 else None
                                    )
                                },
                            )

                            if result.success:
                                success = True
                                break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                            else:
                                error_msg = (
                                    str(result.error_message)
                                    if result.error_message
                                    else "Unknown error"
                                )
                                if (
                                    "ERR_CONNECTION_RESET" in error_msg
                                    or "Connection reset" in error_msg
                                ):
                                    self.logger.warning(
                                        f"âš ï¸ ç¬¬ {page} é¡µè¿æ¥è¢«é‡ç½®ï¼Œå°è¯• {attempt+1}/{max_retries}"
                                    )
                                    if attempt < max_retries - 1:
                                        continue  # ç»§ç»­é‡è¯•
                                else:
                                    self.logger.warning(
                                        f"âš ï¸ ç¬¬ {page} é¡µæŠ“å–å¤±è´¥: {error_msg}"
                                    )
                                    if attempt < max_retries - 1:
                                        continue  # ç»§ç»­é‡è¯•
                                break  # éé‡è¯•ç±»é”™è¯¯ï¼Œç›´æ¥è·³å‡º

                        except Exception as e:
                            error_msg = str(e)
                            if (
                                "ERR_CONNECTION_RESET" in error_msg
                                or "Connection reset" in error_msg
                            ):
                                self.logger.warning(
                                    f"âš ï¸ ç¬¬ {page} é¡µè¿æ¥é‡ç½®å¼‚å¸¸ï¼Œå°è¯• {attempt+1}/{max_retries}"
                                )
                                if attempt < max_retries - 1:
                                    continue  # ç»§ç»­é‡è¯•
                            else:
                                self.logger.error(
                                    f"âŒ ç¬¬ {page} é¡µæŠ“å–å¼‚å¸¸: {error_msg}"
                                )
                                if attempt < max_retries - 1:
                                    continue  # ç»§ç»­é‡è¯•
                            break  # æœ€åä¸€æ¬¡å°è¯•æˆ–éé‡è¯•ç±»é”™è¯¯

                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸæŠ“å–
                    if not success:
                        self.logger.error(
                            f"âŒ ç¬¬ {page} é¡µç»è¿‡ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥"
                        )

                        # å¦‚æœæ˜¯ç¬¬ä¸€é¡µå°±å¤±è´¥ï¼Œæä¾›ç”¨æˆ·æŒ‡å¯¼
                        if page == 1:
                            self.logger.warning(
                                "ğŸš¨ æœç´¢é¡µé¢æŠ“å–è¢«é˜»æ­¢ï¼Œå¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶"
                            )
                            self.logger.info("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                            self.logger.info(
                                "   1. æ‰‹åŠ¨è®¿é—®æœç´¢é¡µé¢ï¼ŒæŸ¥çœ‹æ˜¯å¦éœ€è¦éªŒè¯ç "
                            )
                            self.logger.info(
                                "   2. é€ä¸ªå¤åˆ¶ç§å­è¯¦æƒ…é¡µé¢çš„ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿"
                            )
                            self.logger.info("   3. ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ å•ä¸ªç£åŠ›é“¾æ¥")
                            self.logger.info(f"   4. æœç´¢é¡µé¢: {page_url}")

                            # æ—©æœŸé€€å‡ºï¼Œé¿å…æµªè´¹æ›´å¤šæ—¶é—´
                            break
                        continue  # ç»§ç»­å°è¯•ä¸‹ä¸€é¡µ

                    # è§£æé¡µé¢å†…å®¹
                    try:
                        page_torrents = await self._parse_xxxclub_content(
                            result, search_url
                        )
                        torrents.extend(page_torrents)

                        self.stats["pages_crawled"] += 1
                        self.logger.info(
                            f"âœ… ç¬¬ {page} é¡µè§£æå®Œæˆï¼Œæ‰¾åˆ° {len(page_torrents)} ä¸ªç§å­"
                        )

                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç§å­ï¼Œå¯èƒ½å·²ç»åˆ°æœ€åä¸€é¡µ
                        if not page_torrents:
                            self.logger.info("ğŸ“„ æ²¡æœ‰æ‰¾åˆ°æ›´å¤šç§å­ï¼Œåœæ­¢åˆ†é¡µæŠ“å–")
                            break
                    except Exception as e:
                        self.logger.error(f"âŒ ç¬¬ {page} é¡µè§£æå¤±è´¥: {str(e)}")
                        continue

                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    if page < max_pages:  # ä¸æ˜¯æœ€åä¸€é¡µ
                        await asyncio.sleep(3 + (page % 3))  # 3-5ç§’éšæœºå»¶è¿Ÿ

            self.stats["torrents_found"] = len(torrents)

            if torrents:
                self.logger.info(
                    f"ğŸ¯ æœç´¢é¡µé¢æŠ“å–å®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(torrents)} ä¸ªç§å­"
                )
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç§å­")
                self.logger.info("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                self.logger.info("   - æœç´¢å…³é”®è¯æ— ç»“æœ")
                self.logger.info("   - ç½‘ç«™åçˆ¬è™«æœºåˆ¶é˜»æ­¢è®¿é—®")
                self.logger.info("   - ç½‘é¡µç»“æ„å¯èƒ½å‘ç”Ÿå˜åŒ–")
                self.logger.info("ğŸ’¡ å»ºè®®:")
                self.logger.info("   - æ‰‹åŠ¨è®¿é—®é¡µé¢ç¡®è®¤å†…å®¹")
                self.logger.info("   - å°è¯•å¤åˆ¶å•ä¸ªç£åŠ›é“¾æ¥")

            return torrents

        except Exception as e:
            self.stats["errors"] += 1
            self.logger.error(f"âŒ æŠ“å–æœç´¢é¡µé¢å¤±è´¥: {str(e)}")

            # æä¾›è¯¦ç»†çš„é”™è¯¯æŒ‡å¯¼
            if "ERR_CONNECTION_RESET" in str(e):
                self.logger.info("ğŸš¨ è¿æ¥è¢«é‡ç½®é”™è¯¯é€šå¸¸è¡¨ç¤º:")
                self.logger.info("   - ç½‘ç«™æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
                self.logger.info("   - IPå¯èƒ½è¢«ä¸´æ—¶é™åˆ¶")
                self.logger.info("   - éœ€è¦ä½¿ç”¨VPNæˆ–ä»£ç†")

            self.logger.info("ğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ:")
            self.logger.info("   1. æ‰‹åŠ¨è®¿é—®XXXClubæœç´¢é¡µé¢")
            self.logger.info("   2. å¤åˆ¶å•ä¸ªç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
            self.logger.info("   3. ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ ")

            raise CrawlerError(f"æŠ“å–æœç´¢é¡µé¢å¤±è´¥: {str(e)}") from e

    async def _parse_xxxclub_content(
        self, crawl_result, base_url: str
    ) -> List[TorrentInfo]:
        """è§£æ crawl4ai æŠ“å–çš„å†…å®¹"""
        torrents = []

        try:
            # æš‚æ—¶è·³è¿‡LLMæå–ï¼Œç›´æ¥ä½¿ç”¨ç®€å•è§£ææ–¹æ³•
            # TODO: åç»­é‡æ–°å®ç°LLMæå–åŠŸèƒ½
            self.logger.info("ğŸ“ ä½¿ç”¨BeautifulSoupè§£æç½‘é¡µå†…å®¹")
            return await self._simple_parse_xxxclub(crawl_result.cleaned_html, base_url)

        except Exception as e:
            self.logger.error(f"âŒ è§£æå†…å®¹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§£æ: {str(e)}")
            return await self._simple_parse_xxxclub(crawl_result.cleaned_html, base_url)

    async def _simple_parse_xxxclub(
        self, html_content: str, base_url: str
    ) -> List[TorrentInfo]:
        """ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£ææ–¹æ³•"""
        torrents = []

        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(html_content, "html.parser")

            # æŸ¥æ‰¾æ‰€æœ‰liå…ƒç´ ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªæ ‡é¢˜è¡Œï¼‰
            list_items = soup.find_all("li")

            # è·³è¿‡ç¬¬ä¸€ä¸ªliï¼ˆæ ‡é¢˜è¡Œï¼‰
            if len(list_items) > 1:
                list_items = list_items[1:]

            self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(list_items)} ä¸ªç§å­é¡¹")

            for item in list_items:
                try:
                    # æŸ¥æ‰¾æ‰€æœ‰spanå…ƒç´ 
                    spans = item.find_all("span")
                    if len(spans) < 6:  # è‡³å°‘éœ€è¦6ä¸ªspan
                        continue

                    # ç¬¬äºŒä¸ªspanåŒ…å«ç§å­è¯¦æƒ…é“¾æ¥
                    name_span = spans[1]
                    detail_link_elem = name_span.find(
                        "a", href=lambda x: x and "/torrents/details/" in x
                    )
                    if not detail_link_elem:
                        continue

                    detail_link = detail_link_elem["href"]
                    title = detail_link_elem.get_text(strip=True)

                    if not title:
                        continue

                    # æ„å»ºå®Œæ•´URL
                    if detail_link.startswith("/"):
                        parsed_base = urllib.parse.urlparse(base_url)
                        detail_url = (
                            f"{parsed_base.scheme}://{parsed_base.netloc}{detail_link}"
                        )
                    else:
                        detail_url = detail_link

                    # æå–å…¶ä»–ä¿¡æ¯
                    # ç¬¬å››ä¸ªspanåŒ…å«å¤§å°
                    size = spans[3].get_text(strip=True) if len(spans) > 3 else ""

                    # ç¬¬äº”ä¸ªå’Œç¬¬å…­ä¸ªspanåŒ…å«seederså’Œleechers
                    seeders = 0
                    if len(spans) > 4 and spans[4].get_text(strip=True).isdigit():
                        seeders = int(spans[4].get_text(strip=True))

                    leechers = 0
                    if len(spans) > 5 and spans[5].get_text(strip=True).isdigit():
                        leechers = int(spans[5].get_text(strip=True))

                    torrent_info = TorrentInfo(
                        title=title,
                        detail_url=detail_url,
                        size=size,
                        seeders=seeders,
                        leechers=leechers,
                    )

                    torrents.append(torrent_info)
                    self.logger.debug(
                        f"âœ… è§£æç§å­: {title[:50]}... | å¤§å°: {size} | S/L: {seeders}/{leechers}"
                    )

                except Exception as e:
                    self.logger.debug(f"è§£æç§å­é¡¹å¤±è´¥: {str(e)}")
                    continue

            return torrents

        except Exception as e:
            self.logger.error(f"âŒ ç®€å•è§£æå¤±è´¥: {str(e)}")
            return []

    async def extract_magnet_links(
        self, torrents: List[TorrentInfo]
    ) -> List[TorrentInfo]:
        """ä»ç§å­è¯¦æƒ…é¡µé¢æå–ç£åŠ›é“¾æ¥"""
        self.logger.info(f"ğŸ”— å¼€å§‹æå– {len(torrents)} ä¸ªç§å­çš„ç£åŠ›é“¾æ¥")

        # å¢å¼ºå‹è¯¦æƒ…é¡µçˆ¬è™«é…ç½®
        crawler_config = {
            "verbose": False,
            "browser_type": "chromium",
            "headless": True,
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
            "headers": {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Referer": (
                    base_url if hasattr(self, "base_url") else "https://www.google.com/"
                ),
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "same-origin",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
            },
            "page_timeout": 120000,  # 120ç§’è¶…æ—¶
            "wait_for": 10,  # ç­‰å¾…10ç§’
            "delay_before_return_html": 5,  # è¿”å›å‰ç­‰å¾…5ç§’
            "viewport": {"width": 1920, "height": 1080},
            "timezone_id": "Asia/Shanghai",
            "locale": "zh-CN",
            "extra_http_headers": {
                "DNT": "1",
                "Sec-Ch-Ua": '"Google Chrome";v="125", "Chromium";v="125", "Not.A/Brand";v="24"',
                "Sec-Ch-Ua-Mobile": "?0",
                "Sec-Ch-Ua-Platform": '"Windows"',
            },
        }

        async with AsyncWebCrawler(**crawler_config) as crawler:
            for i, torrent in enumerate(torrents, 1):
                max_retries = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°å¢åŠ åˆ°5æ¬¡
                base_delay = 15  # åŸºç¡€å»¶è¿Ÿ15ç§’
                last_error = None

                for attempt in range(max_retries):
                    try:
                        if attempt > 0:
                            # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                            delay = min(
                                base_delay * (2 ** (attempt - 1))
                                + random.uniform(0, 10),
                                300,
                            )  # æœ€å¤§ä¸è¶…è¿‡5åˆ†é’Ÿ
                            self.logger.info(
                                f"ğŸ”„ [{i}/{len(torrents)}] é‡è¯• {attempt+1}/{max_retries}, ç­‰å¾… {delay:.1f} ç§’: {torrent.title[:50]}..."
                            )
                            await asyncio.sleep(delay)
                        else:
                            self.logger.info(
                                f"ğŸ” [{i}/{len(torrents)}] æå–ç£åŠ›é“¾æ¥: {torrent.title[:50]}..."
                            )

                        # åŠ¨æ€è°ƒæ•´çˆ¬è™«å‚æ•°
                        current_config = crawler_config.copy()
                        if attempt > 1:  # ç¬¬ä¸‰æ¬¡é‡è¯•åå¢åŠ è¶…æ—¶å’Œç­‰å¾…æ—¶é—´
                            current_config["page_timeout"] = 180000  # 180ç§’
                            current_config["wait_for"] = 15
                            current_config["delay_before_return_html"] = 8

                        # ä½¿ç”¨æ™ºèƒ½çˆ¬è™«è®¾ç½®
                        result = await crawler.arun(
                            url=torrent.detail_url,
                            wait_for=current_config["wait_for"],
                            page_timeout=current_config["page_timeout"],
                            bypass_cache=True,
                            delay_before_return_html=current_config[
                                "delay_before_return_html"
                            ],
                            css_selector="body",
                            js_code=(
                                "window.scrollTo(0, document.body.scrollHeight/2);"
                                if attempt > 0
                                else None
                            ),
                            extra_http_headers={
                                "Referer": (
                                    torrent.detail_url
                                    if attempt > 0
                                    else crawler_config["headers"]["Referer"]
                                )
                            },
                        )

                        if not result.success:
                            error_msg = (
                                str(result.error_message)
                                if result.error_message
                                else "Unknown error"
                            )
                            if (
                                "ERR_CONNECTION_RESET" in error_msg
                                or "Connection reset" in error_msg
                            ):
                                self.logger.warning(
                                    f"âš ï¸ è¿æ¥è¢«é‡ç½®ï¼Œå°è¯• {attempt+1}/{max_retries}"
                                )
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(
                                        base_delay * (attempt + 1)
                                    )  # é€’å¢å»¶è¿Ÿ
                                    continue
                            else:
                                self.logger.warning(f"âš ï¸ æŠ“å–å¤±è´¥: {error_msg}")
                                if attempt < max_retries - 1:
                                    await asyncio.sleep(base_delay)
                                    continue
                            break

                        # æå–ç£åŠ›é“¾æ¥å¹¶è§£ææ–‡ä»¶å
                        magnet_link = await self._extract_magnet_from_content(result)

                        if magnet_link and validate_magnet_link(magnet_link):
                            torrent.magnet_link = magnet_link
                            torrent.status = "extracted"
                            self.stats["magnets_extracted"] += 1

                            # æ£€æŸ¥æ˜¯å¦é‡å¤å¹¶è·å–æ–‡ä»¶åï¼ˆä¿ç•™åŸå§‹æ ‡é¢˜ç”¨äºæ˜¾ç¤ºï¼‰
                            torrent_hash, torrent_name = parse_magnet(magnet_link)
                            self.logger.debug(
                                f"ğŸ” ç£åŠ›é“¾æ¥è§£æ - åŸå§‹æ ‡é¢˜: {torrent.title} | è§£ææ–‡ä»¶å: {torrent_name}"
                            )

                            # ä»…å½“ç£åŠ›é“¾æ¥åŒ…å«æ›´å®Œæ•´çš„æ–‡ä»¶åæ—¶æ‰è¦†ç›–
                            if torrent_name and len(torrent_name) > len(torrent.title):
                                torrent.title = torrent_name
                            if torrent_hash and torrent_hash in self.processed_hashes:
                                torrent.status = "duplicate"
                                self.stats["duplicates_skipped"] += 1
                                self.logger.info(
                                    f"âš ï¸ è·³è¿‡é‡å¤ç§å­: {torrent.title[:50]}..."
                                )
                            elif torrent_hash:
                                self.processed_hashes.add(torrent_hash)

                            self.logger.info(
                                f"âœ… æˆåŠŸæå–ç£åŠ›é“¾æ¥: {torrent.title[:30]}..."
                            )
                            break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        else:
                            if attempt < max_retries - 1:
                                self.logger.warning(
                                    f"âš ï¸ æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥ï¼Œé‡è¯• {attempt+1}/{max_retries}"
                                )
                                await asyncio.sleep(retry_delay)
                                continue
                            else:
                                torrent.status = "failed"
                                self.stats["errors"] += 1
                                self.logger.warning(
                                    f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆç£åŠ›é“¾æ¥: {torrent.title[:50]}..."
                                )

                    except Exception as e:
                        error_msg = str(e)
                        if (
                            "ERR_CONNECTION_RESET" in error_msg
                            or "Connection reset" in error_msg
                        ):
                            self.logger.warning(
                                f"âš ï¸ è¿æ¥é‡ç½®é”™è¯¯ï¼Œå°è¯• {attempt+1}/{max_retries}: {torrent.title[:50]}..."
                            )
                            if attempt < max_retries - 1:
                                await asyncio.sleep(base_delay * (attempt + 1))
                                continue
                        else:
                            self.logger.error(
                                f"âŒ æå–å¤±è´¥: {torrent.title[:50]}... - {error_msg}"
                            )
                            if attempt < max_retries - 1:
                                await asyncio.sleep(base_delay)
                                continue

                        # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                        if attempt == max_retries - 1:
                            torrent.status = "failed"
                            self.stats["errors"] += 1
                            break

                # æ¯ä¸ªç§å­é—´çš„å»¶è¿Ÿï¼Œé¿å…è¢«æ£€æµ‹ä¸ºçˆ¬è™«
                if i < len(torrents):  # ä¸æ˜¯æœ€åä¸€ä¸ª
                    await asyncio.sleep(3 + (i % 3))  # 3-5ç§’éšæœºå»¶è¿Ÿ

        successful_extractions = len([t for t in torrents if t.status == "extracted"])
        self.logger.info(
            f"ğŸ¯ ç£åŠ›é“¾æ¥æå–å®Œæˆ: {successful_extractions}/{len(torrents)} æˆåŠŸ"
        )

        return torrents

    async def _extract_magnet_from_content(self, crawl_result) -> Optional[str]:
        """ä»crawl4aiç»“æœä¸­æå–ç£åŠ›é“¾æ¥"""
        try:
            # åœ¨æ–‡æœ¬å†…å®¹ä¸­æœç´¢ç£åŠ›é“¾æ¥
            full_text = crawl_result.markdown + " " + crawl_result.cleaned_html

            # ç£åŠ›é“¾æ¥æ¨¡å¼
            magnet_patterns = [
                r'magnet:\?xt=urn:btih:[0-9a-fA-F]{40}[^"\s]*',
                r'magnet:\?xt=urn:btih:[0-9a-zA-Z]{32}[^"\s]*',
            ]

            for pattern in magnet_patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE)
                if matches:
                    magnet_link = matches[0].replace("&amp;", "&")
                    return magnet_link

            # å¦‚æœæ–‡æœ¬æœç´¢å¤±è´¥ï¼Œä½¿ç”¨HTMLè§£æ
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(crawl_result.cleaned_html, "html.parser")

            # æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
            download_links = soup.find_all("a", href=True)
            for link in download_links:
                href = link["href"]
                if href.startswith("magnet:"):
                    return href.replace("&amp;", "&")

            # æŸ¥æ‰¾æ–‡æœ¬åŒºåŸŸä¸­çš„ç£åŠ›é“¾æ¥
            text_areas = soup.find_all(["textarea", "input"])
            for area in text_areas:
                text = area.get("value", "") or area.get_text()
                if text.startswith("magnet:"):
                    return text.replace("&amp;", "&")

            return None

        except Exception as e:
            self.logger.error(f"æå–ç£åŠ›é“¾æ¥å¤±è´¥: {str(e)}")
            return None

    async def add_torrents_to_qbittorrent(
        self, torrents: List[TorrentInfo]
    ) -> List[TorrentInfo]:
        """
        å°†ç§å­æ‰¹é‡æ·»åŠ åˆ°qBittorrentå¹¶å¤„ç†åˆ†ç±»ã€é‡å‘½åç­‰

        Args:
            torrents: å¾…æ·»åŠ çš„ç§å­ä¿¡æ¯åˆ—è¡¨

        Returns:
            å¤„ç†åçš„ç§å­ä¿¡æ¯åˆ—è¡¨
        """
        if not torrents:
            return []

        self.logger.info(f"â• å¼€å§‹æ‰¹é‡æ·»åŠ  {len(torrents)} ä¸ªç§å­åˆ°qBittorrent...")

        # æ£€æŸ¥å¹¶åˆ›å»ºåŸºç¡€ä¸‹è½½ç›®å½•
        base_download_dir = Path(self.config.qbt.base_download_path)
        if not base_download_dir.exists():
            self.logger.warning(f"åŸºç¡€ä¸‹è½½ç›®å½• {base_download_dir} ä¸å­˜åœ¨ï¼Œå°†å°è¯•åˆ›å»º")
            try:
                base_download_dir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                self.logger.error(f"âŒ åˆ›å»ºåŸºç¡€ä¸‹è½½ç›®å½•å¤±è´¥: {e}")
                for t in torrents:
                    t.status = "failed"
                return torrents

        for torrent in torrents:
            if not torrent.magnet_link or torrent.status != "extracted":
                continue

            try:
                # 1. AIåˆ†ç±»
                if self.config.web_crawler.ai_classify_torrents:
                    self.logger.info(f"ğŸ§  æ­£åœ¨ä¸º '{torrent.title}' è¿›è¡ŒAIåˆ†ç±»...")
                    category, new_name = await self.ai_classifier.classify_torrent(
                        torrent.title
                    )
                    torrent.category = category
                    self.logger.info(f"   - AIåˆ†ç±»ç»“æœ: '{category}'")
                else:
                    torrent.category = "default"

                # 2. è·å–ä¿å­˜è·¯å¾„
                save_path = await self._get_category_save_path(torrent.category)

                # 3. æ·»åŠ ç§å­åˆ°qBittorrent (ä¸è¿›è¡Œé‡å‘½å)
                success = await self.qbt_client.add_torrent(
                    torrent.magnet_link,
                    save_path=save_path,
                    category=torrent.category,
                    paused=self.config.web_crawler.add_torrents_paused,
                )

                if success:
                    self.stats["torrents_added"] += 1
                    torrent.status = "added"
                    self.logger.info(f"âœ… æˆåŠŸæ·»åŠ ç§å­: {torrent.title}")
                else:
                    # å¯èƒ½æ˜¯å› ä¸ºå“ˆå¸Œå·²ç»å­˜åœ¨
                    if await self.qbt_client.is_duplicate(torrent.magnet_link):
                        self.stats["duplicates_skipped"] += 1
                        torrent.status = "duplicate"
                        self.logger.warning(f"âš ï¸ è·³è¿‡é‡å¤çš„ç§å­: {torrent.title}")
                    else:
                        self.stats["errors"] += 1
                        torrent.status = "failed"
                        self.logger.error(f"âŒ æ·»åŠ ç§å­å¤±è´¥: {torrent.title}")

            except Exception as e:
                torrent.status = "failed"
                self.stats["errors"] += 1
                self.logger.error(f"âŒ å¤„ç†ç§å­ '{torrent.title}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
                import traceback

                self.logger.debug(traceback.format_exc())

        # æ‰¹é‡å®Œæˆåå‘é€ä¸€æ¬¡é€šçŸ¥
        await self._notify_completion(torrents)

        return torrents

    async def _get_category_save_path(self, category: str) -> str:
        """è·å–åˆ†ç±»çš„ä¿å­˜è·¯å¾„"""
        try:
            existing_categories = await self.qbt_client.get_existing_categories()
            if category in existing_categories:
                return existing_categories[category].get("savePath", "é»˜è®¤è·¯å¾„")
            elif category in self.config.categories:
                return self.config.categories[category].save_path
        except Exception:
            pass
        return "é»˜è®¤è·¯å¾„"

    def get_stats(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "stats": self.stats.copy(),
            "processed_hashes_count": len(self.processed_hashes),
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            "pages_crawled": 0,
            "torrents_found": 0,
            "magnets_extracted": 0,
            "torrents_added": 0,
            "duplicates_skipped": 0,
            "errors": 0,
        }
        self.processed_hashes.clear()

    async def extract_magnet_links_fallback(
        self, torrents: List[TorrentInfo]
    ) -> List[TorrentInfo]:
        """
        å¤‡ç”¨æ–¹æ³•ï¼šå½“è¯¦æƒ…é¡µé¢æŠ“å–å¤±è´¥æ—¶çš„å¤„ç†æ–¹æ¡ˆ
        æš‚æ—¶æ ‡è®°æ‰€æœ‰ç§å­ä¸ºå¤±è´¥çŠ¶æ€ï¼Œå¹¶æä¾›ç”¨æˆ·æ‰‹åŠ¨æ“ä½œå»ºè®®
        """
        self.logger.warning("ğŸš¨ ç”±äºç½‘ç«™åçˆ¬è™«æœºåˆ¶ï¼Œæš‚æ—¶æ— æ³•è‡ªåŠ¨æå–ç£åŠ›é“¾æ¥")
        self.logger.info("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
        self.logger.info("   1. æ‰‹åŠ¨è®¿é—®ç§å­è¯¦æƒ…é¡µé¢")
        self.logger.info("   2. å¤åˆ¶ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
        self.logger.info("   3. ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ ")

        # æ ‡è®°æ‰€æœ‰ç§å­ä¸ºå¤±è´¥ï¼Œä½†ä¿ç•™ä¿¡æ¯ä¾›ç”¨æˆ·å‚è€ƒ
        for torrent in torrents:
            torrent.status = "failed"
            self.stats["errors"] += 1
            self.logger.info(f"ğŸ“„ ç§å­è¯¦æƒ…: {torrent.title[:60]}")
            self.logger.info(f"   URL: {torrent.detail_url}")

        return torrents


async def crawl_and_add_torrents(
    search_url: str,
    config: AppConfig,
    qbt_client: QBittorrentClient,
    max_pages: int = 1,
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæŠ“å–å¹¶æ·»åŠ ç§å­

    Args:
        search_url: æœç´¢é¡µé¢URL
        config: åº”ç”¨é…ç½®
        qbt_client: qBittorrentå®¢æˆ·ç«¯
        max_pages: æœ€å¤§æŠ“å–é¡µæ•°

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    crawler = WebCrawler(config, qbt_client)

    try:
        # æŠ“å–æœç´¢é¡µé¢
        torrents = await crawler.crawl_xxxclub_search(search_url, max_pages)

        if not torrents:
            # è¯¦ç»†åˆ†æå¤±è´¥åŸå› å¹¶æä¾›æŒ‡å¯¼
            crawler.logger.info("ğŸ” åˆ†æå¯èƒ½çš„é—®é¢˜:")
            crawler.logger.info("   â¤ ç½‘ç«™å¯èƒ½æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®")
            crawler.logger.info("   â¤ æœç´¢å…³é”®è¯å¯èƒ½æ— ç»“æœ")
            crawler.logger.info("   â¤ ç½‘é¡µç»“æ„å¯èƒ½å‘ç”Ÿå˜åŒ–")

            crawler.logger.info("ğŸ› ï¸ å»ºè®®å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("   1. æ‰‹åŠ¨æ‰“å¼€æµè§ˆå™¨è®¿é—®æœç´¢é¡µé¢:")
            crawler.logger.info(f"      {search_url}")
            crawler.logger.info("   2. æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ç æˆ–ç™»å½•")
            crawler.logger.info("   3. å¦‚æœèƒ½æ­£å¸¸è®¿é—®ï¼Œè¯·:")
            crawler.logger.info("      â€¢ æ‰¾åˆ°æ„Ÿå…´è¶£çš„ç§å­")
            crawler.logger.info("      â€¢ ç‚¹å‡»è¿›å…¥ç§å­è¯¦æƒ…é¡µé¢")
            crawler.logger.info("      â€¢ å¤åˆ¶ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
            crawler.logger.info("      â€¢ ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ·»åŠ ")
            crawler.logger.info("   4. å¦‚æœæœç´¢æ— ç»“æœï¼Œå°è¯•ä¿®æ”¹æœç´¢å…³é”®è¯")

            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ä»»ä½•ç§å­ - å¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶æˆ–æœç´¢æ— ç»“æœ",
                "search_url": search_url,
                "suggestions": [
                    "æ‰‹åŠ¨è®¿é—®æœç´¢é¡µé¢ç¡®è®¤å†…å®¹",
                    "å¤åˆ¶å•ä¸ªç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿",
                    "æ£€æŸ¥æœç´¢å…³é”®è¯æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤ç½‘ç«™æ˜¯å¦å¯æ­£å¸¸è®¿é—®",
                ],
                "stats": crawler.get_stats(),
            }

        # æå–ç£åŠ›é“¾æ¥
        torrents = await crawler.extract_magnet_links(torrents)

        # æ£€æŸ¥æå–æˆåŠŸç‡ï¼Œå¦‚æœå¤ªä½åˆ™æä¾›å¤‡ç”¨æ–¹æ¡ˆ
        extracted_count = len([t for t in torrents if t.status == "extracted"])
        total_count = len(torrents)
        success_rate = extracted_count / total_count if total_count > 0 else 0

        if success_rate < 0.1 and total_count > 5:  # æˆåŠŸç‡ä½äº10%ä¸”ç§å­æ•°é‡å¤§äº5
            crawler.logger.warning(
                f"ğŸš¨ ç£åŠ›é“¾æ¥æå–æˆåŠŸç‡è¿‡ä½ ({success_rate:.1%})ï¼Œå¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶"
            )
            crawler.logger.info("ğŸ’¡ ä¸ºæ‚¨æä¾›ç§å­è¯¦æƒ…é¡µé¢é“¾æ¥ï¼Œå¯æ‰‹åŠ¨å¤åˆ¶ç£åŠ›é“¾æ¥:")

            for i, torrent in enumerate(torrents[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                crawler.logger.info(f"[{i}] {torrent.title[:60]}")
                crawler.logger.info(f"    {torrent.detail_url}")
                if i < len(torrents):
                    crawler.logger.info("")

            if len(torrents) > 10:
                crawler.logger.info(f"... è¿˜æœ‰ {len(torrents) - 10} ä¸ªç§å­æœªæ˜¾ç¤º")

            return {
                "success": False,
                "message": f"æŠ“å–å—é˜»: æ‰¾åˆ°{total_count}ä¸ªç§å­ï¼Œä½†åªèƒ½æå–{extracted_count}ä¸ªç£åŠ›é“¾æ¥ã€‚è¯·æ‰‹åŠ¨è®¿é—®è¯¦æƒ…é¡µé¢å¤åˆ¶ç£åŠ›é“¾æ¥ã€‚",
                "torrents": [
                    {
                        "title": t.title,
                        "detail_url": t.detail_url,
                        "status": t.status,
                        "size": t.size,
                    }
                    for t in torrents
                ],
                "stats": crawler.get_stats(),
            }

        # æ·»åŠ åˆ°qBittorrent
        torrents = await crawler.add_torrents_to_qbittorrent(torrents)

        stats = crawler.get_stats()

        return {
            "success": True,
            "message": f"å¤„ç†å®Œæˆ: æ‰¾åˆ°{stats['stats']['torrents_found']}ä¸ªç§å­ï¼ŒæˆåŠŸæ·»åŠ {stats['stats']['torrents_added']}ä¸ª",
            "torrents": [
                {
                    "title": t.title,
                    "status": t.status,
                    "category": t.category,
                    "size": t.size,
                }
                for t in torrents
            ],
            "stats": stats,
        }
    except CrawlerError as e:
        # ä¸“é—¨å¤„ç†çˆ¬è™«é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„æŒ‡å¯¼
        error_msg = str(e)
        crawler.logger.error("ğŸš¨ ç½‘é¡µæŠ“å–é‡åˆ°é—®é¢˜")

        if "ERR_CONNECTION_RESET" in error_msg:
            crawler.logger.info("ğŸ” è¯Šæ–­: è¿æ¥è¢«é‡ç½®")
            crawler.logger.info("ğŸ“‹ è¿™é€šå¸¸è¡¨ç¤º:")
            crawler.logger.info("   â€¢ ç½‘ç«™æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è®¿é—®å¹¶ä¸»åŠ¨æ–­å¼€è¿æ¥")
            crawler.logger.info("   â€¢ IPåœ°å€å¯èƒ½è¢«ä¸´æ—¶é™åˆ¶")
            crawler.logger.info("   â€¢ ç½‘ç«™æœ‰è¾ƒå¼ºçš„åçˆ¬è™«ä¿æŠ¤")

            crawler.logger.info("ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("   1. ç«‹å³è§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("      â€¢ æ‰‹åŠ¨è®¿é—®æœç´¢é¡µé¢")
            crawler.logger.info("      â€¢ å¤åˆ¶ç£åŠ›é“¾æ¥åˆ°å‰ªè´´æ¿")
            crawler.logger.info("      â€¢ ç¨‹åºä¼šè‡ªåŠ¨æ·»åŠ ")
            crawler.logger.info("   2. é•¿æœŸè§£å†³æ–¹æ¡ˆ:")
            crawler.logger.info("      â€¢ ä½¿ç”¨VPNæ¢IP")
            crawler.logger.info("      â€¢ ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•")
            crawler.logger.info("      â€¢ é™ä½è®¿é—®é¢‘ç‡")
        else:
            crawler.logger.info("ğŸ” å…¶ä»–ç½‘ç»œé—®é¢˜")
            crawler.logger.info("ğŸ› ï¸ å»ºè®®:")
            crawler.logger.info("   â€¢ æ£€æŸ¥ç½‘ç»œè¿æ¥")
            crawler.logger.info("   â€¢ ç¡®è®¤ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
            crawler.logger.info("   â€¢ å°è¯•æ‰‹åŠ¨è®¿é—®é¡µé¢")

        return {
            "success": False,
            "message": f"ç½‘é¡µæŠ“å–å¤±è´¥: {error_msg}",
            "error_type": "crawler_error",
            "search_url": search_url,
            "stats": crawler.get_stats(),
        }
    except Exception as e:
        # å¤„ç†å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
        crawler.logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸé”™è¯¯: {str(e)}")
        return {
            "success": False,
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "error_type": "unexpected_error",
            "stats": crawler.get_stats(),
        }
